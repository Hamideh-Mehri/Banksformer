{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data encoding\n",
    "This part takes a pre-processed dataframe, and encodes the data so it can be used to train Banksformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:34:48.143581: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "from datetime import date\n",
    "import tensorflow as tf\n",
    "from prepare_data import preprocess_data_czech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "randint(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_suffix = \"-czech\"\n",
    "max_seq_len = 80\n",
    "min_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../DATA/tr_by_acct_w_age.csv')\n",
    "data = preprocess_data_czech(raw_data)\n",
    "data2 = data[['account_id',  'amount', 'tcode', 'datetime', 'year', 'month', 'dow', 'day', 'dtme','td','age']]\n",
    "df = data2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_FIELDS = ['tcode_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_transform(self, df) from DataEncoder Class\n",
    "# LOG_AMOUNT_SCALE, TD_SCALE, ATTR_SCALE, START_DATE, TCODE_TO_NUM, NUM_TO_TCODE, n_tcodes   are attributes of class.\n",
    "df[\"log_amount\"] = np.log10(df[\"amount\"]+1)\n",
    "LOG_AMOUNT_SCALE = df[\"log_amount\"].std()\n",
    "df[\"log_amount_sc\"] = df[\"log_amount\"] / LOG_AMOUNT_SCALE\n",
    "\n",
    "TD_SCALE = df[\"td\"].std()\n",
    "df[\"td_sc\"] = df[\"td\"] / TD_SCALE\n",
    "\n",
    "ATTR_SCALE = df[\"age\"].std()\n",
    "df[\"age_sc\"] = df[\"age\"] / ATTR_SCALE\n",
    "\n",
    "START_DATE = df[\"datetime\"].min()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in CAT_FIELDS:\n",
    "    field = field.replace(\"_num\", \"\")\n",
    "    cat_to_num = dict([(tc, i) for i, tc in enumerate(df[field].unique())])\n",
    "    TCODE_TO_NUM = cat_to_num\n",
    "    NUM_TO_TCODE = dict([(i, tc) for i, tc in enumerate(df[field].unique())])\n",
    "    df[field + \"_num\"] = df[field].apply(lambda x: cat_to_num[x])\n",
    "    n_tcodes = len(cat_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>tcode</th>\n",
       "      <th>datetime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dow</th>\n",
       "      <th>day</th>\n",
       "      <th>dtme</th>\n",
       "      <th>td</th>\n",
       "      <th>age</th>\n",
       "      <th>log_amount</th>\n",
       "      <th>log_amount_sc</th>\n",
       "      <th>td_sc</th>\n",
       "      <th>age_sc</th>\n",
       "      <th>tcode_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>CREDIT__CREDIT IN CASH__nan</td>\n",
       "      <td>1995-03-24</td>\n",
       "      <td>1995</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>3.000434</td>\n",
       "      <td>2.823750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>CREDIT__COLLECTION FROM ANOTHER BANK__nan</td>\n",
       "      <td>1995-04-13</td>\n",
       "      <td>1995</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>20.0</td>\n",
       "      <td>29</td>\n",
       "      <td>3.565848</td>\n",
       "      <td>3.355869</td>\n",
       "      <td>3.298201</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>12600.0</td>\n",
       "      <td>CREDIT__CREDIT IN CASH__nan</td>\n",
       "      <td>1995-04-23</td>\n",
       "      <td>1995</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29</td>\n",
       "      <td>4.100405</td>\n",
       "      <td>3.858949</td>\n",
       "      <td>1.649100</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>19.2</td>\n",
       "      <td>CREDIT__nan__INTEREST CREDITED</td>\n",
       "      <td>1995-04-30</td>\n",
       "      <td>1995</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.305351</td>\n",
       "      <td>1.228484</td>\n",
       "      <td>1.154370</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>CREDIT__COLLECTION FROM ANOTHER BANK__nan</td>\n",
       "      <td>1995-05-13</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>29</td>\n",
       "      <td>3.565848</td>\n",
       "      <td>3.355869</td>\n",
       "      <td>2.143831</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056315</th>\n",
       "      <td>11382</td>\n",
       "      <td>25600.0</td>\n",
       "      <td>DEBIT__CASH WITHDRAWAL__nan</td>\n",
       "      <td>1998-12-02</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46</td>\n",
       "      <td>4.408257</td>\n",
       "      <td>4.148672</td>\n",
       "      <td>0.329820</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056316</th>\n",
       "      <td>11382</td>\n",
       "      <td>46248.0</td>\n",
       "      <td>CREDIT__COLLECTION FROM ANOTHER BANK__nan</td>\n",
       "      <td>1998-12-10</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>8.0</td>\n",
       "      <td>46</td>\n",
       "      <td>4.665102</td>\n",
       "      <td>4.390393</td>\n",
       "      <td>1.319280</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056317</th>\n",
       "      <td>11382</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>DEBIT__CASH WITHDRAWAL__nan</td>\n",
       "      <td>1998-12-25</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.799409</td>\n",
       "      <td>3.575677</td>\n",
       "      <td>2.473651</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056318</th>\n",
       "      <td>11382</td>\n",
       "      <td>311.3</td>\n",
       "      <td>CREDIT__nan__INTEREST CREDITED</td>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.494572</td>\n",
       "      <td>2.347677</td>\n",
       "      <td>0.989460</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056319</th>\n",
       "      <td>11382</td>\n",
       "      <td>301.1</td>\n",
       "      <td>CREDIT__nan__INTEREST CREDITED</td>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.480151</td>\n",
       "      <td>2.334105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056320 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         account_id   amount                                      tcode  \\\n",
       "0                 1   1000.0                CREDIT__CREDIT IN CASH__nan   \n",
       "1                 1   3679.0  CREDIT__COLLECTION FROM ANOTHER BANK__nan   \n",
       "2                 1  12600.0                CREDIT__CREDIT IN CASH__nan   \n",
       "3                 1     19.2             CREDIT__nan__INTEREST CREDITED   \n",
       "4                 1   3679.0  CREDIT__COLLECTION FROM ANOTHER BANK__nan   \n",
       "...             ...      ...                                        ...   \n",
       "1056315       11382  25600.0                DEBIT__CASH WITHDRAWAL__nan   \n",
       "1056316       11382  46248.0  CREDIT__COLLECTION FROM ANOTHER BANK__nan   \n",
       "1056317       11382   6300.0                DEBIT__CASH WITHDRAWAL__nan   \n",
       "1056318       11382    311.3             CREDIT__nan__INTEREST CREDITED   \n",
       "1056319       11382    301.1             CREDIT__nan__INTEREST CREDITED   \n",
       "\n",
       "          datetime  year  month  dow  day  dtme    td  age  log_amount  \\\n",
       "0       1995-03-24  1995      3    4   24     7   0.0   29    3.000434   \n",
       "1       1995-04-13  1995      4    3   13    17  20.0   29    3.565848   \n",
       "2       1995-04-23  1995      4    6   23     7  10.0   29    4.100405   \n",
       "3       1995-04-30  1995      4    6   30     0   7.0   29    1.305351   \n",
       "4       1995-05-13  1995      5    5   13    18  13.0   29    3.565848   \n",
       "...            ...   ...    ...  ...  ...   ...   ...  ...         ...   \n",
       "1056315 1998-12-02  1998     12    2    2    29   2.0   46    4.408257   \n",
       "1056316 1998-12-10  1998     12    3   10    21   8.0   46    4.665102   \n",
       "1056317 1998-12-25  1998     12    4   25     6  15.0   46    3.799409   \n",
       "1056318 1998-12-31  1998     12    3   31     0   6.0   46    2.494572   \n",
       "1056319 1998-12-31  1998     12    3   31     0   0.0   46    2.480151   \n",
       "\n",
       "         log_amount_sc     td_sc    age_sc  tcode_num  \n",
       "0             2.823750  0.000000  1.745524          0  \n",
       "1             3.355869  3.298201  1.745524          1  \n",
       "2             3.858949  1.649100  1.745524          0  \n",
       "3             1.228484  1.154370  1.745524          2  \n",
       "4             3.355869  2.143831  1.745524          1  \n",
       "...                ...       ...       ...        ...  \n",
       "1056315       4.148672  0.329820  2.768763          3  \n",
       "1056316       4.390393  1.319280  2.768763          1  \n",
       "1056317       3.575677  2.473651  2.768763          3  \n",
       "1056318       2.347677  0.989460  2.768763          2  \n",
       "1056319       2.334105  0.000000  2.768763          2  \n",
       "\n",
       "[1056320 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONT_FIELDS = ['log_amount_sc']\n",
    "DATE_FIELDS = ['dow', 'month', \"day\", 'dtme', 'td_sc']  \n",
    "DATA_KEY_ORDER = CAT_FIELDS + DATE_FIELDS + CONT_FIELDS\n",
    "date_loss = \"scce\"\n",
    "\n",
    "LOSS_TYPES = {\"day\": date_loss,\n",
    "              \"dtme\": date_loss,\n",
    "           \"dow\": date_loss,\n",
    "           \"month\": date_loss,\n",
    "           \"tcode_num\": date_loss,\n",
    "            \"td_sc\": \"pdf\",\n",
    "            \"log_amount_sc\": \"pdf\",\n",
    "             }\n",
    "CLOCK_DIMS = {\"day\": 31,\n",
    "              \"dtme\": 31,\n",
    "           \"dow\": 7,\n",
    "           \"month\": 12,}\n",
    "\n",
    "# cl - clock encoding (2d)\n",
    "# oh - One-hot encoding\n",
    "# raw - no encoding\n",
    "# cl-i -  clock integer: transforms [1, 2, ..., n] -> [1, 2, ..., n-1, 0]\n",
    "INP_ENCODINGS = {\"day\": \"cl\",\n",
    "                 \"dtme\": \"cl\",\n",
    "           \"dow\": \"cl\",\n",
    "           \"month\": \"cl\",\n",
    "            \"td_sc\": \"raw\",\n",
    "            \"log_amount_sc\": \"raw\",\n",
    "            \"tcode_num\": \"oh\",}\n",
    "\n",
    "TAR_ENCODINGS = {\"day\": \"cl-i\",\n",
    "                 \"dtme\": \"cl-i\",\n",
    "           \"dow\": \"cl-i\",\n",
    "           \"month\": \"cl-i\",\n",
    "            \"td_sc\": \"raw\",\n",
    "            \"log_amount_sc\": \"raw\",\n",
    "            \"tcode_num\": \"raw\",}\n",
    "\n",
    "FIELD_DIMS_IN  = {}\n",
    "FIELD_DIMS_TAR = {}\n",
    "FIELD_DIMS_NET = {}\n",
    "\n",
    "\n",
    "ENCODING_INP_DIMS_BY_TYPE = {'cl':2, \n",
    "                             'oh':None, \n",
    "                             'raw':1}\n",
    "\n",
    "ENCODING_TAR_DIMS_BY_TYPE = {'cl-i': 1, \n",
    "                             'raw': 1}\n",
    "\n",
    "# ENCODING_NET_DIMS_BY_TYPE \n",
    "\n",
    "for k in DATA_KEY_ORDER:\n",
    "    \n",
    "    FIELD_DIMS_IN[k] = ENCODING_INP_DIMS_BY_TYPE[INP_ENCODINGS[k]]\n",
    "    FIELD_DIMS_TAR[k] = ENCODING_TAR_DIMS_BY_TYPE[TAR_ENCODINGS[k]]\n",
    "    \n",
    "    if TAR_ENCODINGS[k] == \"raw\":\n",
    "        FIELD_DIMS_NET[k] = 2\n",
    "    elif TAR_ENCODINGS[k] == \"cl-i\":\n",
    "        FIELD_DIMS_NET[k] = CLOCK_DIMS[k]\n",
    "    else:\n",
    "        raise Exception(f\"Error getting network dim for field = {k}\")\n",
    "# field = 'tcode'  \n",
    "for field in CAT_FIELDS:      \n",
    "    FIELD_DIMS_IN[field] = n_tcodes\n",
    "    FIELD_DIMS_NET[field] = n_tcodes\n",
    "\n",
    "FIELD_STARTS_IN = {}\n",
    "start = 0\n",
    "for k in DATA_KEY_ORDER:\n",
    "\n",
    "    FIELD_STARTS_IN[k] = start\n",
    "    start += FIELD_DIMS_IN[k]\n",
    "\n",
    "\n",
    "\n",
    "FIELD_STARTS_TAR = {}\n",
    "start = 0\n",
    "for k in DATA_KEY_ORDER:\n",
    "\n",
    "    FIELD_STARTS_TAR[k] = start\n",
    "    start += FIELD_DIMS_TAR[k]\n",
    "    \n",
    "    \n",
    "FIELD_STARTS_NET = {}\n",
    "start = 0\n",
    "for k in DATA_KEY_ORDER:\n",
    "\n",
    "    FIELD_STARTS_NET[k] = start\n",
    "    start += FIELD_DIMS_NET[k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': 16,\n",
       " 'dow': 2,\n",
       " 'month': 2,\n",
       " 'day': 2,\n",
       " 'dtme': 2,\n",
       " 'td_sc': 1,\n",
       " 'log_amount_sc': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_DIMS_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': 0,\n",
       " 'dow': 16,\n",
       " 'month': 18,\n",
       " 'day': 20,\n",
       " 'dtme': 22,\n",
       " 'td_sc': 24,\n",
       " 'log_amount_sc': 25}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_STARTS_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': 1,\n",
       " 'dow': 1,\n",
       " 'month': 1,\n",
       " 'day': 1,\n",
       " 'dtme': 1,\n",
       " 'td_sc': 1,\n",
       " 'log_amount_sc': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_DIMS_TAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': 0,\n",
       " 'dow': 1,\n",
       " 'month': 2,\n",
       " 'day': 3,\n",
       " 'dtme': 4,\n",
       " 'td_sc': 5,\n",
       " 'log_amount_sc': 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_STARTS_TAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': 16,\n",
       " 'dow': 7,\n",
       " 'month': 12,\n",
       " 'day': 31,\n",
       " 'dtme': 31,\n",
       " 'td_sc': 2,\n",
       " 'log_amount_sc': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_DIMS_NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': 0,\n",
       " 'dow': 16,\n",
       " 'month': 23,\n",
       " 'day': 35,\n",
       " 'dtme': 66,\n",
       " 'td_sc': 97,\n",
       " 'log_amount_sc': 99}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_STARTS_NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcode_num', 'dow', 'month', 'day', 'dtme', 'td_sc', 'log_amount_sc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_KEY_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_seqs_in_df(df):                   #14354\n",
    "    gb_aid = df.groupby(\"account_id\")[\"account_id\"]\n",
    "\n",
    "    full_seqs_per_acct = gb_aid.count() // max_seq_len\n",
    "\n",
    "    n_full_seqs = sum(full_seqs_per_acct)\n",
    "    n_part_seqs = sum(gb_aid.count() - full_seqs_per_acct*max_seq_len >= min_seq_len)\n",
    "    \n",
    "    return n_full_seqs + n_part_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14354, 80, 26), (14354, 80, 7))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seqs = count_seqs_in_df(df)\n",
    "n_steps = max_seq_len\n",
    "n_feat_inp = sum(FIELD_DIMS_IN.values())\n",
    "n_feat_tar = sum(FIELD_DIMS_TAR.values())\n",
    "\n",
    "inp_tensor = np.zeros((n_seqs, n_steps, n_feat_inp))\n",
    "tar_tensor = np.zeros((n_seqs, n_steps, n_feat_tar))\n",
    "\n",
    "inp_tensor.shape, tar_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_encode_time_value(val, max_val):\n",
    "    x = np.sin(2* np.pi/max_val * val)\n",
    "    y = np.cos(2*np.pi /max_val * val)\n",
    "    return np.stack([x,y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq is group.iloc[start:start+seq_len], group is df.groupby('account_id')\n",
    "def seq_to_inp_tensor(seq, inp_tensor, seq_i, seq_len):\n",
    "\n",
    "    for k in DATA_KEY_ORDER:\n",
    "\n",
    "        depth = FIELD_DIMS_IN[k]\n",
    "        st = FIELD_STARTS_IN[k]\n",
    "        enc_type = INP_ENCODINGS[k]\n",
    "        \n",
    "        if enc_type == \"oh\":\n",
    "            x = tf.one_hot(seq[k], depth).numpy()\n",
    "        elif enc_type == \"cl\":\n",
    "            max_val = CLOCK_DIMS[k]\n",
    "            x = bulk_encode_time_value(seq[k], max_val)\n",
    "        elif enc_type == \"raw\":\n",
    "            x = np.expand_dims(seq[k], 1)\n",
    "        else:\n",
    "            raise Exception(f\"Got invalid enc_type: {enc_type}\")\n",
    "            \n",
    "            \n",
    "\n",
    "        inp_tensor[seq_i,:seq_len, st:st+depth] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_targ_tensor(seq, tar_tensor, seq_i, seq_len):\n",
    "\n",
    "    for k in DATA_KEY_ORDER:\n",
    "        \n",
    "        depth = FIELD_DIMS_TAR[k]\n",
    "        st = FIELD_STARTS_TAR[k]\n",
    "        enc_type = TAR_ENCODINGS[k]\n",
    "        \n",
    "        if enc_type == \"cl-i\":\n",
    "            max_val = CLOCK_DIMS[k]\n",
    "            x = np.expand_dims(seq[k]%max_val, 1)\n",
    "        elif enc_type == \"raw\":\n",
    "            x = np.expand_dims(seq[k], 1)\n",
    "        else:\n",
    "            raise Exception(f\"Got invalid enc_type: {enc_type}\")\n",
    "            \n",
    "        \n",
    "        tar_tensor[seq_i,:seq_len, st:st+depth] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 10:51:42.805963: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-10-25 10:51:42.807168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-10-25 10:51:42.836128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.45GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2023-10-25 10:51:42.836371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:af:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.45GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2023-10-25 10:51:42.836402: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-10-25 10:51:42.853868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-10-25 10:51:42.853924: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-10-25 10:51:42.855632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-10-25 10:51:42.855893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-10-25 10:51:42.857494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-10-25 10:51:42.858334: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-10-25 10:51:42.861734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-10-25 10:51:42.862321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-10-25 10:51:42.877451: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-25 10:51:42.878217: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-10-25 10:51:43.059419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.45GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2023-10-25 10:51:43.059603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:af:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 47.45GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2023-10-25 10:51:43.059644: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-10-25 10:51:43.059664: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-10-25 10:51:43.059673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-10-25 10:51:43.059682: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-10-25 10:51:43.059691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-10-25 10:51:43.059700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-10-25 10:51:43.059708: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-10-25 10:51:43.059717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-10-25 10:51:43.060236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-10-25 10:51:43.060265: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-10-25 10:51:44.425532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-10-25 10:51:44.425560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2023-10-25 10:51:44.425565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2023-10-25 10:51:44.425568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2023-10-25 10:51:44.426435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 37192 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:3b:00.0, compute capability: 7.5)\n",
      "2023-10-25 10:51:44.427013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 45387 MB memory) -> physical GPU (device: 1, name: Quadro RTX 8000, pci bus id: 0000:af:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished encoding 2000 of 14354 seqs\n",
      "Finished encoding 4000 of 14354 seqs\n",
      "Finished encoding 6000 of 14354 seqs\n",
      "Finished encoding 8000 of 14354 seqs\n",
      "Finished encoding 10000 of 14354 seqs\n",
      "Finished encoding 12000 of 14354 seqs\n",
      "Finished encoding 14000 of 14354 seqs\n",
      "Took 90.38 secs\n"
     ]
    }
   ],
   "source": [
    "seq_i = 0\n",
    "rows_per_acct = {}\n",
    "alert_every = 2000\n",
    "attribute = \"age_sc\"\n",
    "\n",
    "\n",
    "attributes = np.zeros(n_seqs)\n",
    "start_time = time.time()\n",
    "for acct_id, group in df.groupby(\"account_id\"):\n",
    "    rows_per_acct[acct_id] = []\n",
    "    \n",
    "    for i in range(len(group) // max_seq_len + 1):\n",
    "\n",
    "        n_trs = len(group)\n",
    "        start = i*max_seq_len\n",
    "        seq_len = min(max_seq_len, n_trs - start)   \n",
    "\n",
    "        if seq_len >= min_seq_len:\n",
    "            seq_to_inp_tensor(group.iloc[start:start+seq_len], inp_tensor, seq_i, seq_len)\n",
    "            seq_to_targ_tensor(group.iloc[start:start+seq_len],tar_tensor, seq_i, seq_len)\n",
    "#             tar_tensor[seq_i,:seq_len,:] = seq_to_targ_tensor(group.iloc[start:start+seq_len])\n",
    "            attributes[seq_i] = group[\"age\"].iloc[0]\n",
    "\n",
    "            rows_per_acct[acct_id].append(seq_i)\n",
    "            seq_i += 1\n",
    "            \n",
    "            if seq_i % alert_every == 0:\n",
    "                print(f\"Finished encoding {seq_i} of {n_seqs} seqs\")\n",
    "inp_tensor = np.concatenate([np.repeat(attributes[:, None, None], n_feat_inp, axis=2), \n",
    "                             inp_tensor], \n",
    "                             axis=1)\n",
    "print(f\"Took {time.time() - start_time:.2f} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29.,\n",
       "       29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tensor[0][0]    # the first element in the sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14354, 80, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([29., 29., 29., 54., 54., 54., 54., 54., 54., 43.]), (14354,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes[:10], attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [0, 1, 2],\n",
       " 2: [3, 4, 5, 6, 7, 8],\n",
       " 3: [9, 10],\n",
       " 4: [11, 12, 13],\n",
       " 5: [14],\n",
       " 6: [15, 16, 17],\n",
       " 7: [18, 19],\n",
       " 8: [20, 21, 22],\n",
       " 9: [23, 24, 25, 26, 27],\n",
       " 10: [28, 29],\n",
       " 11: [30, 31, 32],\n",
       " 12: [33, 34],\n",
       " 13: [35],\n",
       " 14: [36, 37],\n",
       " 15: [38, 39, 40, 41, 42],\n",
       " 16: [43],\n",
       " 17: [44],\n",
       " 18: [45, 46, 47, 48],\n",
       " 19: [49, 50, 51, 52],\n",
       " 20: [53, 54],\n",
       " 21: [55],\n",
       " 22: [56, 57, 58, 59],\n",
       " 23: [60, 61, 62, 63, 64],\n",
       " 24: [65, 66],\n",
       " 25: [67, 68, 69, 70],\n",
       " 26: [71, 72, 73, 74, 75],\n",
       " 27: [76, 77, 78, 79, 80],\n",
       " 29: [81, 82, 83, 84, 85],\n",
       " 30: [86, 87, 88, 89],\n",
       " 31: [90, 91, 92, 93, 94],\n",
       " 32: [95, 96, 97],\n",
       " 33: [98, 99, 100, 101, 102],\n",
       " 34: [103, 104, 105, 106, 107],\n",
       " 35: [108, 109, 110, 111, 112],\n",
       " 36: [113, 114, 115, 116, 117],\n",
       " 37: [118, 119],\n",
       " 38: [120, 121],\n",
       " 39: [122, 123, 124, 125, 126],\n",
       " 40: [127, 128, 129, 130, 131],\n",
       " 41: [132],\n",
       " 42: [133, 134, 135],\n",
       " 43: [136, 137, 138, 139],\n",
       " 44: [140, 141, 142, 143],\n",
       " 45: [144],\n",
       " 47: [145, 146, 147, 148, 149, 150, 151],\n",
       " 48: [152, 153],\n",
       " 49: [154],\n",
       " 50: [155, 156, 157, 158, 159],\n",
       " 51: [160, 161, 162],\n",
       " 52: [163, 164],\n",
       " 53: [165, 166, 167],\n",
       " 54: [168, 169, 170],\n",
       " 55: [171, 172],\n",
       " 56: [173, 174, 175, 176],\n",
       " 57: [177, 178, 179],\n",
       " 58: [180, 181],\n",
       " 59: [182, 183, 184, 185, 186],\n",
       " 61: [187, 188, 189, 190],\n",
       " 62: [191, 192, 193, 194, 195],\n",
       " 63: [196, 197],\n",
       " 64: [198],\n",
       " 65: [199, 200, 201],\n",
       " 66: [202, 203, 204, 205],\n",
       " 67: [206, 207, 208, 209, 210],\n",
       " 68: [211, 212, 213, 214, 215, 216],\n",
       " 69: [217, 218, 219],\n",
       " 70: [220, 221, 222],\n",
       " 71: [223, 224, 225, 226, 227, 228],\n",
       " 72: [229, 230, 231],\n",
       " 73: [232, 233, 234, 235, 236],\n",
       " 75: [237, 238, 239],\n",
       " 76: [240, 241],\n",
       " 77: [242, 243],\n",
       " 78: [244, 245, 246, 247, 248],\n",
       " 79: [249, 250],\n",
       " 80: [251, 252],\n",
       " 81: [253, 254, 255],\n",
       " 82: [256, 257, 258, 259],\n",
       " 83: [260, 261, 262, 263, 264, 265],\n",
       " 84: [266, 267, 268],\n",
       " 85: [269, 270],\n",
       " 86: [271, 272, 273, 274, 275],\n",
       " 88: [276, 277],\n",
       " 89: [278],\n",
       " 90: [279, 280, 281, 282, 283, 284],\n",
       " 91: [285],\n",
       " 93: [286, 287],\n",
       " 95: [288, 289, 290],\n",
       " 96: [291, 292, 293, 294, 295, 296, 297, 298, 299],\n",
       " 97: [300, 301, 302, 303],\n",
       " 98: [304, 305],\n",
       " 99: [306, 307, 308],\n",
       " 100: [309, 310, 311, 312, 313],\n",
       " 101: [314, 315, 316],\n",
       " 102: [317, 318, 319, 320],\n",
       " 103: [321, 322, 323],\n",
       " 104: [324, 325, 326, 327, 328],\n",
       " 105: [329],\n",
       " 106: [330, 331, 332, 333],\n",
       " 107: [334, 335, 336, 337, 338, 339],\n",
       " 108: [340, 341, 342],\n",
       " 109: [343, 344],\n",
       " 110: [345, 346, 347],\n",
       " 112: [348, 349, 350, 351, 352],\n",
       " 113: [353, 354],\n",
       " 114: [355, 356],\n",
       " 115: [357, 358, 359, 360, 361],\n",
       " 116: [362, 363],\n",
       " 117: [364, 365, 366, 367],\n",
       " 118: [368],\n",
       " 119: [369, 370, 371, 372, 373],\n",
       " 120: [374, 375, 376],\n",
       " 121: [377],\n",
       " 122: [378, 379],\n",
       " 123: [380, 381, 382, 383],\n",
       " 124: [384],\n",
       " 125: [385, 386],\n",
       " 126: [387, 388, 389, 390],\n",
       " 127: [391, 392, 393, 394],\n",
       " 128: [395, 396, 397, 398, 399],\n",
       " 129: [400],\n",
       " 130: [401, 402],\n",
       " 131: [403, 404, 405],\n",
       " 132: [406, 407, 408],\n",
       " 133: [409, 410],\n",
       " 134: [411],\n",
       " 135: [412, 413, 414, 415],\n",
       " 136: [416, 417],\n",
       " 137: [418, 419, 420],\n",
       " 138: [421],\n",
       " 139: [422],\n",
       " 140: [423],\n",
       " 141: [424, 425, 426],\n",
       " 142: [427, 428, 429, 430, 431],\n",
       " 143: [432, 433],\n",
       " 144: [434, 435],\n",
       " 145: [436, 437, 438],\n",
       " 146: [439, 440, 441, 442],\n",
       " 148: [443, 444, 445, 446, 447, 448],\n",
       " 149: [449],\n",
       " 150: [450, 451, 452],\n",
       " 151: [453, 454, 455, 456, 457],\n",
       " 152: [458, 459, 460],\n",
       " 153: [461, 462, 463, 464],\n",
       " 154: [465, 466, 467, 468],\n",
       " 155: [469, 470, 471],\n",
       " 156: [472],\n",
       " 157: [473],\n",
       " 158: [474, 475, 476, 477],\n",
       " 159: [478, 479],\n",
       " 160: [480, 481, 482, 483, 484, 485],\n",
       " 161: [486, 487, 488, 489],\n",
       " 162: [490, 491, 492, 493],\n",
       " 163: [494, 495, 496, 497, 498, 499],\n",
       " 164: [500],\n",
       " 165: [501, 502, 503, 504, 505],\n",
       " 166: [506, 507],\n",
       " 167: [508, 509],\n",
       " 168: [510, 511, 512, 513],\n",
       " 169: [514, 515, 516],\n",
       " 170: [517, 518],\n",
       " 171: [519, 520],\n",
       " 172: [521, 522, 523, 524, 525],\n",
       " 173: [526, 527, 528, 529, 530, 531, 532],\n",
       " 174: [533, 534],\n",
       " 176: [535, 536, 537, 538],\n",
       " 177: [539],\n",
       " 178: [540],\n",
       " 179: [541, 542],\n",
       " 180: [543, 544, 545, 546],\n",
       " 181: [547, 548],\n",
       " 182: [],\n",
       " 183: [549, 550, 551, 552, 553],\n",
       " 184: [554, 555],\n",
       " 185: [556],\n",
       " 186: [557, 558, 559, 560, 561],\n",
       " 187: [562, 563, 564, 565],\n",
       " 188: [566, 567, 568, 569, 570],\n",
       " 189: [571, 572],\n",
       " 190: [573, 574, 575, 576, 577],\n",
       " 191: [578, 579],\n",
       " 192: [580, 581, 582, 583, 584],\n",
       " 193: [585, 586],\n",
       " 194: [587, 588],\n",
       " 195: [589, 590],\n",
       " 196: [591, 592, 593, 594, 595, 596, 597],\n",
       " 197: [598, 599, 600, 601, 602],\n",
       " 198: [603, 604, 605, 606],\n",
       " 199: [607, 608],\n",
       " 200: [609, 610, 611],\n",
       " 201: [612, 613, 614, 615],\n",
       " 204: [616, 617],\n",
       " 205: [618],\n",
       " 206: [619, 620, 621],\n",
       " 207: [622, 623, 624, 625, 626, 627],\n",
       " 208: [628, 629, 630, 631],\n",
       " 209: [632, 633],\n",
       " 210: [634, 635, 636],\n",
       " 211: [637],\n",
       " 212: [638, 639, 640, 641, 642, 643],\n",
       " 213: [644, 645, 646],\n",
       " 214: [647],\n",
       " 215: [648],\n",
       " 216: [649, 650, 651, 652, 653],\n",
       " 217: [654, 655, 656],\n",
       " 218: [657, 658, 659],\n",
       " 219: [660, 661, 662],\n",
       " 220: [663, 664, 665, 666],\n",
       " 221: [667, 668],\n",
       " 222: [669, 670, 671, 672, 673],\n",
       " 223: [674, 675],\n",
       " 224: [676, 677],\n",
       " 225: [678, 679, 680, 681, 682, 683],\n",
       " 226: [684, 685],\n",
       " 227: [686, 687, 688],\n",
       " 228: [689, 690, 691, 692, 693],\n",
       " 229: [694],\n",
       " 230: [695, 696, 697],\n",
       " 231: [698, 699, 700],\n",
       " 232: [701],\n",
       " 234: [702, 703, 704, 705],\n",
       " 235: [706, 707, 708, 709, 710],\n",
       " 236: [711],\n",
       " 237: [712, 713, 714, 715],\n",
       " 238: [716, 717, 718, 719],\n",
       " 239: [720, 721, 722, 723, 724, 725, 726],\n",
       " 240: [727, 728],\n",
       " 241: [729, 730, 731],\n",
       " 242: [732, 733],\n",
       " 243: [734, 735, 736, 737, 738],\n",
       " 244: [739, 740, 741, 742, 743, 744, 745],\n",
       " 245: [746, 747, 748, 749],\n",
       " 246: [750],\n",
       " 248: [751, 752, 753],\n",
       " 249: [754, 755],\n",
       " 250: [756],\n",
       " 251: [757, 758, 759],\n",
       " 252: [760, 761],\n",
       " 253: [762, 763],\n",
       " 255: [764, 765],\n",
       " 256: [766, 767, 768],\n",
       " 257: [769, 770],\n",
       " 258: [771],\n",
       " 259: [772, 773, 774, 775, 776],\n",
       " 260: [777, 778, 779],\n",
       " 261: [780, 781],\n",
       " 262: [782],\n",
       " 263: [783, 784],\n",
       " 264: [785, 786, 787],\n",
       " 265: [788, 789],\n",
       " 266: [790, 791, 792, 793],\n",
       " 267: [794, 795, 796],\n",
       " 268: [797, 798],\n",
       " 269: [799, 800],\n",
       " 270: [801, 802, 803],\n",
       " 271: [804, 805],\n",
       " 272: [806, 807],\n",
       " 273: [808, 809, 810, 811, 812, 813],\n",
       " 274: [814, 815, 816, 817, 818],\n",
       " 275: [819, 820, 821],\n",
       " 276: [822],\n",
       " 277: [823, 824, 825, 826],\n",
       " 278: [827, 828, 829],\n",
       " 279: [830, 831, 832, 833, 834],\n",
       " 280: [835, 836, 837, 838, 839, 840],\n",
       " 281: [841, 842],\n",
       " 282: [843, 844],\n",
       " 283: [845, 846],\n",
       " 284: [847],\n",
       " 285: [848, 849, 850],\n",
       " 286: [851, 852],\n",
       " 287: [853, 854, 855, 856, 857],\n",
       " 288: [858, 859, 860],\n",
       " 289: [861, 862, 863, 864, 865],\n",
       " 290: [866, 867, 868, 869, 870, 871],\n",
       " 291: [872, 873],\n",
       " 292: [874],\n",
       " 293: [875, 876],\n",
       " 294: [877, 878],\n",
       " 295: [879],\n",
       " 296: [880, 881],\n",
       " 297: [882, 883],\n",
       " 298: [884],\n",
       " 299: [885, 886, 887, 888, 889],\n",
       " 300: [890, 891, 892, 893, 894],\n",
       " 301: [895, 896],\n",
       " 302: [897, 898],\n",
       " 303: [899, 900],\n",
       " 304: [901, 902],\n",
       " 305: [903, 904],\n",
       " 306: [905, 906, 907, 908, 909],\n",
       " 307: [910],\n",
       " 308: [911],\n",
       " 309: [912, 913],\n",
       " 311: [914, 915],\n",
       " 312: [916, 917, 918],\n",
       " 313: [919, 920],\n",
       " 314: [921, 922, 923],\n",
       " 315: [],\n",
       " 316: [924, 925],\n",
       " 317: [926, 927, 928, 929, 930],\n",
       " 318: [931, 932, 933, 934],\n",
       " 319: [935, 936, 937, 938, 939],\n",
       " 320: [940, 941],\n",
       " 321: [942, 943, 944, 945],\n",
       " 322: [946, 947, 948, 949, 950],\n",
       " 323: [951, 952],\n",
       " 324: [953, 954],\n",
       " 325: [955, 956],\n",
       " 326: [957],\n",
       " 327: [958, 959, 960, 961, 962, 963],\n",
       " 328: [964],\n",
       " 329: [965, 966],\n",
       " 330: [967, 968],\n",
       " 331: [969, 970, 971],\n",
       " 332: [972, 973, 974],\n",
       " 333: [975, 976],\n",
       " 334: [977, 978, 979, 980, 981, 982],\n",
       " 335: [983, 984],\n",
       " 336: [985, 986, 987],\n",
       " 337: [988, 989, 990],\n",
       " 338: [991, 992],\n",
       " 339: [993, 994, 995],\n",
       " 340: [996, 997, 998, 999, 1000],\n",
       " 341: [1001, 1002],\n",
       " 342: [1003, 1004],\n",
       " 343: [1005, 1006, 1007, 1008],\n",
       " 344: [1009, 1010, 1011, 1012, 1013],\n",
       " 345: [1014, 1015, 1016, 1017, 1018],\n",
       " 346: [1019, 1020, 1021, 1022, 1023],\n",
       " 347: [1024, 1025, 1026, 1027, 1028],\n",
       " 348: [1029, 1030],\n",
       " 349: [1031, 1032],\n",
       " 350: [1033, 1034],\n",
       " 351: [1035, 1036],\n",
       " 352: [1037, 1038, 1039, 1040],\n",
       " 353: [1041, 1042, 1043],\n",
       " 354: [1044, 1045, 1046, 1047, 1048],\n",
       " 355: [1049, 1050, 1051],\n",
       " 356: [1052, 1053],\n",
       " 357: [1054, 1055, 1056],\n",
       " 358: [1057, 1058, 1059, 1060, 1061, 1062],\n",
       " 359: [1063, 1064, 1065],\n",
       " 360: [1066, 1067],\n",
       " 361: [1068, 1069, 1070, 1071, 1072, 1073],\n",
       " 362: [1074, 1075, 1076, 1077, 1078, 1079, 1080],\n",
       " 363: [1081, 1082, 1083, 1084],\n",
       " 364: [1085],\n",
       " 365: [1086, 1087, 1088, 1089, 1090, 1091],\n",
       " 366: [1092, 1093, 1094],\n",
       " 367: [1095],\n",
       " 368: [1096, 1097, 1098, 1099, 1100, 1101, 1102],\n",
       " 370: [1103, 1104, 1105, 1106, 1107],\n",
       " 371: [1108, 1109],\n",
       " 372: [1110, 1111, 1112],\n",
       " 373: [1113, 1114],\n",
       " 374: [1115, 1116, 1117, 1118, 1119],\n",
       " 375: [1120, 1121],\n",
       " 376: [1122, 1123, 1124, 1125],\n",
       " 377: [1126, 1127, 1128, 1129],\n",
       " 378: [1130, 1131, 1132, 1133, 1134],\n",
       " 379: [1135, 1136, 1137],\n",
       " 381: [1138, 1139, 1140, 1141],\n",
       " 382: [1142, 1143],\n",
       " 383: [1144],\n",
       " 384: [1145],\n",
       " 385: [1146, 1147, 1148, 1149, 1150, 1151],\n",
       " 386: [1152],\n",
       " 387: [1153],\n",
       " 388: [1154, 1155, 1156, 1157],\n",
       " 389: [1158, 1159, 1160, 1161, 1162],\n",
       " 390: [1163, 1164, 1165, 1166],\n",
       " 391: [1167, 1168, 1169, 1170, 1171],\n",
       " 392: [1172, 1173, 1174],\n",
       " 393: [1175, 1176, 1177, 1178],\n",
       " 395: [1179, 1180, 1181, 1182, 1183, 1184],\n",
       " 396: [1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192],\n",
       " 397: [1193, 1194, 1195, 1196],\n",
       " 398: [1197],\n",
       " 399: [1198, 1199],\n",
       " 400: [1200, 1201, 1202, 1203, 1204],\n",
       " 401: [1205, 1206, 1207, 1208, 1209, 1210],\n",
       " 402: [1211],\n",
       " 403: [1212, 1213, 1214, 1215],\n",
       " 404: [1216, 1217, 1218, 1219],\n",
       " 405: [1220, 1221],\n",
       " 406: [1222, 1223],\n",
       " 407: [1224, 1225],\n",
       " 408: [1226, 1227, 1228, 1229, 1230],\n",
       " 409: [1231, 1232],\n",
       " 410: [1233, 1234],\n",
       " 411: [1235, 1236, 1237, 1238, 1239],\n",
       " 412: [1240, 1241, 1242],\n",
       " 413: [1243, 1244],\n",
       " 414: [1245, 1246, 1247],\n",
       " 415: [1248, 1249, 1250, 1251, 1252],\n",
       " 416: [1253, 1254],\n",
       " 417: [1255, 1256, 1257],\n",
       " 418: [1258, 1259, 1260],\n",
       " 419: [1261, 1262, 1263, 1264],\n",
       " 420: [1265, 1266, 1267],\n",
       " 421: [1268],\n",
       " 422: [1269, 1270, 1271],\n",
       " 423: [1272, 1273, 1274, 1275],\n",
       " 424: [1276, 1277],\n",
       " 425: [1278, 1279, 1280, 1281, 1282, 1283, 1284],\n",
       " 426: [1285, 1286, 1287, 1288, 1289, 1290],\n",
       " 427: [1291, 1292, 1293, 1294],\n",
       " 428: [1295, 1296, 1297, 1298, 1299],\n",
       " 429: [1300, 1301, 1302],\n",
       " 430: [1303],\n",
       " 431: [1304, 1305],\n",
       " 432: [1306, 1307, 1308, 1309],\n",
       " 433: [1310, 1311, 1312],\n",
       " 434: [1313, 1314],\n",
       " 435: [1315, 1316, 1317, 1318],\n",
       " 436: [1319, 1320, 1321],\n",
       " 437: [1322, 1323, 1324, 1325, 1326, 1327],\n",
       " 438: [1328],\n",
       " 439: [1329, 1330],\n",
       " 440: [1331, 1332],\n",
       " 441: [1333, 1334, 1335, 1336, 1337, 1338],\n",
       " 442: [1339],\n",
       " 443: [1340, 1341, 1342, 1343, 1344, 1345],\n",
       " 444: [1346, 1347],\n",
       " 445: [1348, 1349, 1350, 1351, 1352],\n",
       " 446: [1353, 1354, 1355, 1356, 1357],\n",
       " 447: [1358, 1359],\n",
       " 449: [1360, 1361, 1362, 1363],\n",
       " 450: [1364, 1365],\n",
       " 451: [1366, 1367],\n",
       " 452: [1368, 1369, 1370],\n",
       " 453: [1371, 1372],\n",
       " 454: [1373, 1374, 1375],\n",
       " 455: [1376, 1377, 1378],\n",
       " 456: [1379, 1380, 1381, 1382, 1383],\n",
       " 458: [1384, 1385, 1386],\n",
       " 459: [1387],\n",
       " 460: [1388, 1389, 1390],\n",
       " 461: [1391, 1392, 1393, 1394, 1395],\n",
       " 462: [1396, 1397, 1398, 1399, 1400],\n",
       " 463: [1401, 1402, 1403],\n",
       " 464: [1404],\n",
       " 465: [1405, 1406, 1407, 1408, 1409],\n",
       " 466: [1410],\n",
       " 467: [1411, 1412, 1413, 1414, 1415],\n",
       " 468: [1416, 1417, 1418],\n",
       " 469: [1419],\n",
       " 470: [1420, 1421, 1422, 1423, 1424, 1425],\n",
       " 471: [1426, 1427, 1428, 1429, 1430],\n",
       " 472: [1431, 1432],\n",
       " 473: [1433, 1434, 1435],\n",
       " 474: [1436, 1437, 1438],\n",
       " 475: [1439],\n",
       " 476: [],\n",
       " 477: [1440, 1441, 1442, 1443, 1444, 1445, 1446],\n",
       " 478: [1447, 1448, 1449, 1450],\n",
       " 479: [1451, 1452],\n",
       " 480: [1453, 1454],\n",
       " 481: [1455, 1456, 1457],\n",
       " 483: [1458],\n",
       " 484: [1459, 1460, 1461, 1462, 1463],\n",
       " 485: [1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471],\n",
       " 486: [1472, 1473, 1474],\n",
       " 487: [1475, 1476, 1477],\n",
       " 488: [1478],\n",
       " 489: [1479, 1480, 1481, 1482, 1483],\n",
       " 490: [1484, 1485, 1486],\n",
       " 491: [1487, 1488],\n",
       " 492: [1489, 1490, 1491, 1492],\n",
       " 493: [1493, 1494, 1495, 1496],\n",
       " 494: [1497, 1498, 1499, 1500, 1501, 1502],\n",
       " 496: [1503, 1504, 1505, 1506, 1507],\n",
       " 497: [1508, 1509, 1510, 1511, 1512],\n",
       " 498: [1513, 1514, 1515, 1516],\n",
       " 500: [1517, 1518, 1519],\n",
       " 501: [1520, 1521, 1522],\n",
       " 502: [1523],\n",
       " 503: [1524, 1525],\n",
       " 504: [1526, 1527, 1528, 1529],\n",
       " 505: [1530, 1531],\n",
       " 506: [1532, 1533, 1534],\n",
       " 507: [1535, 1536],\n",
       " 508: [1537, 1538],\n",
       " 509: [1539],\n",
       " 510: [1540, 1541, 1542, 1543, 1544, 1545],\n",
       " 511: [1546, 1547],\n",
       " 512: [1548, 1549],\n",
       " 513: [1550, 1551],\n",
       " 514: [1552, 1553],\n",
       " 515: [1554, 1555, 1556],\n",
       " 516: [1557, 1558],\n",
       " 517: [1559, 1560, 1561, 1562, 1563],\n",
       " 518: [1564, 1565, 1566, 1567],\n",
       " 519: [1568, 1569, 1570, 1571],\n",
       " 520: [1572, 1573, 1574, 1575],\n",
       " 521: [1576, 1577, 1578, 1579],\n",
       " 522: [1580, 1581, 1582],\n",
       " 523: [1583, 1584, 1585, 1586],\n",
       " 524: [1587, 1588, 1589, 1590],\n",
       " 525: [1591, 1592, 1593],\n",
       " 526: [1594, 1595],\n",
       " 528: [1596],\n",
       " 529: [1597],\n",
       " 530: [1598, 1599],\n",
       " 531: [1600, 1601, 1602],\n",
       " 532: [1603, 1604, 1605],\n",
       " 533: [1606, 1607, 1608],\n",
       " 534: [1609, 1610],\n",
       " 535: [1611, 1612, 1613, 1614, 1615],\n",
       " 536: [1616, 1617],\n",
       " 537: [1618, 1619],\n",
       " 538: [1620, 1621, 1622],\n",
       " 539: [1623, 1624, 1625, 1626, 1627],\n",
       " 540: [1628, 1629, 1630, 1631, 1632],\n",
       " 541: [1633, 1634, 1635, 1636, 1637, 1638],\n",
       " 542: [1639, 1640, 1641, 1642],\n",
       " 543: [1643, 1644, 1645, 1646, 1647],\n",
       " 544: [1648, 1649, 1650],\n",
       " 545: [1651, 1652, 1653, 1654],\n",
       " 546: [1655, 1656],\n",
       " 547: [1657],\n",
       " 548: [1658, 1659, 1660, 1661, 1662],\n",
       " 549: [1663, 1664],\n",
       " 550: [1665],\n",
       " 551: [1666, 1667, 1668, 1669],\n",
       " 552: [1670, 1671, 1672, 1673],\n",
       " 553: [1674, 1675, 1676, 1677],\n",
       " 554: [1678, 1679, 1680],\n",
       " 555: [1681, 1682, 1683, 1684, 1685, 1686, 1687],\n",
       " 556: [1688],\n",
       " 557: [1689, 1690],\n",
       " 558: [1691, 1692, 1693],\n",
       " 559: [1694, 1695, 1696, 1697, 1698],\n",
       " 560: [1699, 1700, 1701],\n",
       " 561: [1702, 1703],\n",
       " 562: [1704, 1705, 1706, 1707, 1708, 1709],\n",
       " 563: [1710, 1711, 1712, 1713, 1714],\n",
       " 564: [1715],\n",
       " 565: [1716, 1717],\n",
       " 566: [1718],\n",
       " 567: [1719, 1720],\n",
       " 568: [1721, 1722, 1723],\n",
       " 569: [1724, 1725],\n",
       " 570: [1726, 1727],\n",
       " 571: [1728, 1729],\n",
       " 572: [1730, 1731, 1732, 1733, 1734, 1735],\n",
       " 573: [1736],\n",
       " 574: [1737, 1738, 1739, 1740, 1741],\n",
       " 575: [1742, 1743, 1744, 1745, 1746],\n",
       " 576: [1747, 1748, 1749, 1750, 1751],\n",
       " 577: [1752, 1753, 1754, 1755],\n",
       " 578: [1756, 1757],\n",
       " 579: [1758],\n",
       " 580: [1759, 1760, 1761],\n",
       " 581: [1762, 1763],\n",
       " 582: [1764, 1765, 1766],\n",
       " 584: [1767, 1768],\n",
       " 585: [1769],\n",
       " 586: [1770, 1771, 1772],\n",
       " 587: [1773],\n",
       " 588: [1774, 1775, 1776, 1777, 1778, 1779, 1780],\n",
       " 589: [1781, 1782],\n",
       " 590: [1783, 1784],\n",
       " 591: [1785],\n",
       " 592: [1786, 1787],\n",
       " 593: [1788, 1789],\n",
       " 594: [1790, 1791, 1792],\n",
       " 595: [1793, 1794],\n",
       " 596: [1795, 1796, 1797],\n",
       " 597: [1798, 1799, 1800, 1801, 1802],\n",
       " 598: [1803, 1804],\n",
       " 599: [1805, 1806],\n",
       " 600: [1807, 1808],\n",
       " 601: [1809, 1810, 1811, 1812],\n",
       " 602: [1813, 1814, 1815],\n",
       " 603: [1816, 1817],\n",
       " 604: [1818, 1819, 1820, 1821],\n",
       " 605: [1822, 1823],\n",
       " 606: [1824],\n",
       " 607: [1825, 1826, 1827],\n",
       " 608: [1828, 1829, 1830],\n",
       " 609: [1831, 1832, 1833, 1834, 1835],\n",
       " 610: [1836, 1837, 1838],\n",
       " 611: [1839],\n",
       " 612: [1840, 1841],\n",
       " 613: [1842, 1843],\n",
       " 614: [1844],\n",
       " 615: [1845, 1846, 1847],\n",
       " 616: [1848, 1849, 1850, 1851],\n",
       " 617: [1852, 1853],\n",
       " 618: [1854, 1855],\n",
       " 619: [1856, 1857, 1858, 1859],\n",
       " 620: [1860],\n",
       " 621: [1861, 1862, 1863],\n",
       " 622: [1864, 1865],\n",
       " 623: [1866, 1867, 1868, 1869, 1870],\n",
       " 624: [1871, 1872, 1873, 1874, 1875],\n",
       " 625: [1876, 1877, 1878],\n",
       " 626: [1879, 1880, 1881, 1882, 1883],\n",
       " 627: [1884, 1885, 1886, 1887, 1888, 1889],\n",
       " 628: [1890, 1891, 1892, 1893],\n",
       " 629: [1894, 1895, 1896, 1897],\n",
       " 630: [1898, 1899, 1900, 1901, 1902],\n",
       " 631: [1903, 1904, 1905],\n",
       " 632: [1906, 1907, 1908],\n",
       " 633: [1909, 1910],\n",
       " 635: [1911, 1912, 1913, 1914, 1915, 1916],\n",
       " 636: [1917],\n",
       " 637: [1918, 1919],\n",
       " 638: [1920, 1921, 1922, 1923],\n",
       " 639: [1924],\n",
       " 640: [1925, 1926, 1927, 1928],\n",
       " 641: [1929],\n",
       " 643: [1930],\n",
       " 644: [1931],\n",
       " 645: [1932, 1933],\n",
       " 646: [1934, 1935, 1936],\n",
       " 647: [1937, 1938],\n",
       " 648: [1939, 1940, 1941, 1942, 1943],\n",
       " 649: [1944, 1945, 1946, 1947],\n",
       " 650: [1948, 1949, 1950],\n",
       " 651: [1951, 1952, 1953, 1954, 1955],\n",
       " 652: [1956, 1957, 1958, 1959, 1960],\n",
       " 653: [1961],\n",
       " 654: [1962, 1963, 1964],\n",
       " 655: [1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972],\n",
       " 656: [1973, 1974],\n",
       " 657: [1975, 1976, 1977, 1978, 1979, 1980],\n",
       " 658: [1981, 1982],\n",
       " 659: [1983, 1984],\n",
       " 660: [1985, 1986, 1987],\n",
       " 662: [1988, 1989, 1990],\n",
       " 664: [1991, 1992],\n",
       " 665: [1993, 1994, 1995],\n",
       " 666: [1996, 1997, 1998, 1999],\n",
       " 669: [2000, 2001],\n",
       " 671: [2002],\n",
       " 672: [],\n",
       " 673: [2003],\n",
       " 674: [2004, 2005, 2006],\n",
       " 675: [2007, 2008, 2009, 2010],\n",
       " 676: [2011, 2012, 2013],\n",
       " 677: [2014],\n",
       " 678: [2015, 2016, 2017, 2018, 2019, 2020],\n",
       " 679: [2021, 2022],\n",
       " 680: [2023],\n",
       " 682: [2024],\n",
       " 683: [2025, 2026],\n",
       " 684: [2027, 2028],\n",
       " 685: [2029, 2030, 2031],\n",
       " 686: [2032, 2033, 2034, 2035, 2036, 2037, 2038],\n",
       " 687: [2039, 2040, 2041],\n",
       " 688: [2042, 2043, 2044, 2045, 2046],\n",
       " 689: [2047, 2048, 2049, 2050],\n",
       " 690: [2051, 2052],\n",
       " 691: [2053, 2054],\n",
       " 692: [2055, 2056],\n",
       " 693: [2057, 2058, 2059, 2060],\n",
       " 694: [2061, 2062],\n",
       " 695: [2063, 2064],\n",
       " 696: [2065],\n",
       " 697: [2066, 2067, 2068, 2069, 2070],\n",
       " 698: [2071, 2072],\n",
       " 699: [2073, 2074, 2075, 2076, 2077],\n",
       " 700: [2078, 2079, 2080, 2081, 2082],\n",
       " 701: [2083, 2084, 2085, 2086, 2087],\n",
       " 702: [2088, 2089, 2090, 2091],\n",
       " 703: [2092],\n",
       " 704: [2093, 2094, 2095, 2096, 2097, 2098],\n",
       " 705: [2099],\n",
       " 706: [2100, 2101],\n",
       " 708: [2102],\n",
       " 709: [2103, 2104, 2105, 2106],\n",
       " 710: [2107, 2108, 2109],\n",
       " 711: [2110, 2111, 2112, 2113],\n",
       " 712: [2114, 2115],\n",
       " 713: [2116, 2117],\n",
       " 714: [2118, 2119],\n",
       " 715: [2120, 2121, 2122],\n",
       " 716: [2123, 2124, 2125],\n",
       " 717: [2126, 2127, 2128, 2129, 2130],\n",
       " 718: [2131, 2132, 2133, 2134],\n",
       " 719: [2135, 2136, 2137, 2138, 2139],\n",
       " 720: [2140, 2141, 2142],\n",
       " 721: [2143, 2144, 2145, 2146],\n",
       " 722: [2147],\n",
       " 725: [2148, 2149],\n",
       " 726: [2150, 2151, 2152, 2153],\n",
       " 727: [],\n",
       " 728: [2154, 2155, 2156],\n",
       " 729: [2157, 2158],\n",
       " 730: [2159, 2160, 2161, 2162, 2163],\n",
       " 731: [2164, 2165, 2166],\n",
       " 732: [2167, 2168],\n",
       " 733: [2169, 2170, 2171, 2172],\n",
       " 734: [2173, 2174],\n",
       " 735: [2175],\n",
       " 736: [2176],\n",
       " 737: [2177, 2178, 2179, 2180],\n",
       " 738: [2181, 2182, 2183, 2184, 2185, 2186, 2187],\n",
       " 739: [2188, 2189],\n",
       " 740: [2190, 2191],\n",
       " 741: [2192, 2193, 2194, 2195],\n",
       " 742: [2196, 2197, 2198, 2199],\n",
       " 743: [2200, 2201, 2202, 2203],\n",
       " 744: [2204, 2205, 2206, 2207, 2208],\n",
       " 745: [2209],\n",
       " 746: [2210, 2211, 2212],\n",
       " 747: [2213, 2214, 2215, 2216, 2217],\n",
       " 748: [2218, 2219, 2220, 2221, 2222],\n",
       " 749: [2223, 2224],\n",
       " 750: [2225, 2226, 2227, 2228, 2229],\n",
       " 751: [2230, 2231, 2232, 2233],\n",
       " 752: [2234, 2235],\n",
       " 753: [2236, 2237],\n",
       " 754: [2238, 2239, 2240],\n",
       " 755: [2241],\n",
       " 756: [2242],\n",
       " 757: [2243, 2244, 2245, 2246, 2247],\n",
       " 758: [],\n",
       " 759: [2248, 2249, 2250, 2251, 2252],\n",
       " 760: [2253, 2254],\n",
       " 761: [2255],\n",
       " 762: [2256, 2257, 2258],\n",
       " 763: [2259, 2260, 2261, 2262],\n",
       " 764: [2263, 2264, 2265, 2266, 2267],\n",
       " 765: [2268, 2269],\n",
       " 766: [2270, 2271, 2272],\n",
       " 767: [2273, 2274, 2275, 2276, 2277],\n",
       " 768: [2278],\n",
       " 770: [2279, 2280],\n",
       " 771: [2281, 2282, 2283],\n",
       " 772: [2284, 2285, 2286, 2287, 2288, 2289, 2290],\n",
       " 773: [2291, 2292, 2293, 2294, 2295, 2296],\n",
       " 774: [2297, 2298, 2299, 2300, 2301],\n",
       " 775: [2302, 2303, 2304, 2305],\n",
       " 776: [2306, 2307, 2308],\n",
       " 777: [2309],\n",
       " 778: [2310],\n",
       " 779: [2311, 2312],\n",
       " 780: [2313, 2314],\n",
       " 781: [2315, 2316, 2317],\n",
       " 782: [2318],\n",
       " 783: [2319],\n",
       " 784: [2320, 2321, 2322, 2323, 2324, 2325],\n",
       " 785: [2326, 2327],\n",
       " 786: [2328, 2329, 2330, 2331],\n",
       " 787: [2332, 2333, 2334, 2335, 2336],\n",
       " 788: [2337, 2338],\n",
       " 789: [2339, 2340, 2341, 2342],\n",
       " 790: [2343, 2344, 2345, 2346, 2347],\n",
       " 791: [2348, 2349],\n",
       " 792: [2350, 2351],\n",
       " 793: [2352, 2353, 2354, 2355, 2356, 2357],\n",
       " 794: [2358],\n",
       " 795: [2359, 2360, 2361, 2362],\n",
       " 796: [2363],\n",
       " 797: [2364, 2365],\n",
       " 798: [2366, 2367, 2368],\n",
       " 799: [2369],\n",
       " 800: [2370],\n",
       " 801: [2371],\n",
       " 802: [2372, 2373, 2374],\n",
       " 803: [2375, 2376, 2377],\n",
       " 804: [2378, 2379, 2380],\n",
       " 805: [2381, 2382],\n",
       " 806: [2383, 2384],\n",
       " 807: [2385, 2386, 2387],\n",
       " 808: [2388, 2389, 2390, 2391],\n",
       " 809: [2392, 2393, 2394],\n",
       " 810: [2395, 2396, 2397],\n",
       " 811: [2398],\n",
       " 812: [2399, 2400],\n",
       " 813: [2401, 2402, 2403, 2404, 2405],\n",
       " 814: [2406, 2407],\n",
       " 815: [2408, 2409, 2410, 2411],\n",
       " 816: [2412, 2413, 2414, 2415, 2416, 2417],\n",
       " 817: [2418, 2419, 2420, 2421, 2422],\n",
       " 819: [2423, 2424, 2425],\n",
       " 820: [2426, 2427, 2428],\n",
       " 821: [2429, 2430, 2431],\n",
       " 822: [2432, 2433],\n",
       " 823: [2434],\n",
       " 824: [2435, 2436, 2437],\n",
       " 825: [2438, 2439, 2440],\n",
       " 826: [2441, 2442],\n",
       " 827: [2443, 2444],\n",
       " 828: [2445, 2446],\n",
       " 829: [2447, 2448],\n",
       " 830: [2449, 2450, 2451, 2452],\n",
       " 831: [2453, 2454],\n",
       " 832: [2455, 2456],\n",
       " 833: [2457, 2458],\n",
       " 834: [2459],\n",
       " 836: [2460, 2461],\n",
       " 837: [2462, 2463, 2464, 2465, 2466],\n",
       " 838: [2467],\n",
       " 839: [2468, 2469, 2470],\n",
       " 840: [2471, 2472, 2473],\n",
       " 841: [2474, 2475, 2476, 2477],\n",
       " 842: [2478, 2479, 2480],\n",
       " 843: [2481, 2482],\n",
       " 844: [2483, 2484, 2485, 2486, 2487],\n",
       " 845: [2488],\n",
       " 846: [2489, 2490, 2491],\n",
       " 847: [2492, 2493],\n",
       " 848: [2494, 2495, 2496],\n",
       " 849: [2497, 2498],\n",
       " 850: [2499, 2500],\n",
       " 851: [2501, 2502, 2503, 2504, 2505],\n",
       " 852: [2506],\n",
       " 853: [2507, 2508, 2509],\n",
       " 854: [2510, 2511, 2512, 2513, 2514, 2515, 2516],\n",
       " 855: [2517, 2518, 2519, 2520],\n",
       " 856: [2521, 2522, 2523, 2524],\n",
       " 857: [2525, 2526, 2527, 2528],\n",
       " 858: [2529, 2530],\n",
       " 859: [2531, 2532],\n",
       " 860: [2533, 2534, 2535, 2536, 2537],\n",
       " 861: [2538, 2539, 2540, 2541, 2542],\n",
       " 862: [2543, 2544, 2545, 2546, 2547, 2548],\n",
       " 863: [2549, 2550, 2551, 2552, 2553, 2554],\n",
       " 864: [2555],\n",
       " 865: [2556, 2557],\n",
       " 866: [2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565],\n",
       " 867: [2566],\n",
       " 868: [2567],\n",
       " 869: [],\n",
       " 870: [2568, 2569, 2570],\n",
       " 871: [2571, 2572],\n",
       " 872: [2573, 2574],\n",
       " 873: [2575, 2576],\n",
       " 874: [2577, 2578, 2579],\n",
       " 875: [2580, 2581, 2582],\n",
       " 876: [2583],\n",
       " 877: [2584, 2585, 2586],\n",
       " 878: [2587, 2588],\n",
       " 879: [2589],\n",
       " 880: [2590, 2591],\n",
       " 882: [2592, 2593, 2594, 2595, 2596],\n",
       " 883: [2597, 2598, 2599, 2600, 2601, 2602],\n",
       " 884: [2603, 2604, 2605, 2606],\n",
       " 885: [2607, 2608, 2609, 2610, 2611, 2612],\n",
       " 886: [2613, 2614, 2615, 2616, 2617],\n",
       " 887: [2618, 2619, 2620, 2621],\n",
       " 888: [2622, 2623],\n",
       " 889: [2624],\n",
       " 890: [2625, 2626, 2627, 2628, 2629],\n",
       " 891: [2630, 2631],\n",
       " 892: [2632, 2633, 2634, 2635, 2636],\n",
       " 893: [2637, 2638, 2639],\n",
       " 894: [2640, 2641, 2642],\n",
       " 895: [2643, 2644, 2645],\n",
       " 896: [2646, 2647, 2648],\n",
       " 897: [2649, 2650, 2651, 2652, 2653, 2654],\n",
       " 898: [2655, 2656, 2657],\n",
       " 899: [2658, 2659, 2660, 2661],\n",
       " 900: [2662, 2663, 2664, 2665],\n",
       " 901: [2666, 2667, 2668, 2669],\n",
       " 902: [2670, 2671, 2672, 2673, 2674, 2675],\n",
       " 903: [2676, 2677, 2678, 2679],\n",
       " 904: [2680, 2681, 2682, 2683],\n",
       " 905: [2684, 2685],\n",
       " 906: [2686],\n",
       " 907: [2687, 2688, 2689, 2690, 2691],\n",
       " 908: [2692, 2693, 2694, 2695],\n",
       " 909: [2696],\n",
       " 910: [2697, 2698],\n",
       " 911: [2699, 2700],\n",
       " 912: [2701],\n",
       " 913: [2702, 2703, 2704],\n",
       " 914: [2705, 2706, 2707, 2708],\n",
       " 915: [2709, 2710, 2711, 2712],\n",
       " 916: [2713, 2714],\n",
       " 917: [2715, 2716, 2717],\n",
       " 918: [2718, 2719],\n",
       " 919: [2720, 2721],\n",
       " 920: [2722, 2723],\n",
       " 921: [2724, 2725],\n",
       " 922: [2726, 2727, 2728, 2729],\n",
       " 923: [2730, 2731, 2732],\n",
       " 924: [2733, 2734, 2735],\n",
       " 925: [2736, 2737, 2738, 2739, 2740],\n",
       " 926: [2741, 2742],\n",
       " 927: [2743, 2744],\n",
       " 928: [2745, 2746, 2747, 2748, 2749],\n",
       " 929: [2750, 2751, 2752, 2753, 2754],\n",
       " 930: [2755, 2756, 2757, 2758, 2759],\n",
       " 931: [2760, 2761, 2762, 2763, 2764, 2765],\n",
       " 932: [2766, 2767],\n",
       " 933: [2768, 2769, 2770],\n",
       " 934: [2771],\n",
       " 935: [2772, 2773],\n",
       " 936: [2774, 2775, 2776],\n",
       " 937: [2777, 2778, 2779],\n",
       " 938: [2780],\n",
       " 939: [2781, 2782, 2783],\n",
       " 940: [2784, 2785, 2786, 2787],\n",
       " 941: [2788, 2789, 2790, 2791],\n",
       " 942: [2792, 2793, 2794, 2795, 2796],\n",
       " 943: [2797, 2798, 2799, 2800, 2801],\n",
       " 944: [2802, 2803, 2804, 2805, 2806],\n",
       " 945: [2807],\n",
       " 946: [2808, 2809],\n",
       " 947: [2810, 2811, 2812, 2813, 2814, 2815],\n",
       " 949: [2816, 2817],\n",
       " 950: [2818, 2819, 2820, 2821, 2822],\n",
       " 951: [2823, 2824],\n",
       " 952: [2825],\n",
       " 953: [2826, 2827, 2828, 2829],\n",
       " 954: [2830, 2831, 2832],\n",
       " 955: [2833, 2834],\n",
       " 956: [2835, 2836, 2837, 2838],\n",
       " 957: [2839, 2840, 2841],\n",
       " 958: [2842, 2843],\n",
       " 959: [2844, 2845, 2846],\n",
       " 960: [2847, 2848, 2849, 2850, 2851],\n",
       " 961: [2852],\n",
       " 962: [2853, 2854],\n",
       " 963: [2855, 2856],\n",
       " 964: [2857, 2858, 2859],\n",
       " 965: [2860, 2861, 2862, 2863, 2864],\n",
       " 966: [2865],\n",
       " 967: [2866, 2867],\n",
       " 968: [2868, 2869, 2870],\n",
       " 969: [2871, 2872, 2873, 2874, 2875],\n",
       " 970: [2876],\n",
       " 971: [2877, 2878],\n",
       " 972: [2879, 2880],\n",
       " 973: [2881, 2882, 2883],\n",
       " 974: [2884, 2885, 2886, 2887],\n",
       " 975: [2888, 2889],\n",
       " 976: [2890, 2891, 2892],\n",
       " 977: [2893, 2894, 2895, 2896, 2897],\n",
       " 978: [2898, 2899, 2900, 2901],\n",
       " 979: [2902, 2903, 2904, 2905, 2906],\n",
       " 980: [2907, 2908],\n",
       " 981: [2909, 2910, 2911, 2912],\n",
       " 982: [2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920],\n",
       " 984: [2921, 2922],\n",
       " 985: [2923, 2924, 2925],\n",
       " 986: [2926, 2927],\n",
       " 987: [2928, 2929],\n",
       " 988: [2930, 2931, 2932, 2933],\n",
       " 989: [2934, 2935, 2936, 2937],\n",
       " 990: [2938, 2939, 2940],\n",
       " 991: [2941, 2942, 2943, 2944, 2945],\n",
       " 992: [2946, 2947, 2948, 2949],\n",
       " 993: [2950, 2951],\n",
       " 994: [2952, 2953, 2954],\n",
       " 995: [2955, 2956, 2957],\n",
       " 996: [2958],\n",
       " 997: [2959, 2960],\n",
       " 998: [2961, 2962, 2963, 2964, 2965],\n",
       " 1000: [2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973],\n",
       " 1002: [2974, 2975],\n",
       " 1003: [2976, 2977],\n",
       " 1004: [2978, 2979, 2980],\n",
       " 1005: [2981, 2982, 2983, 2984, 2985, 2986],\n",
       " 1006: [2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994],\n",
       " 1007: [2995, 2996, 2997],\n",
       " 1008: [2998, 2999],\n",
       " 1009: [3000, 3001, 3002],\n",
       " 1010: [3003, 3004, 3005, 3006],\n",
       " 1011: [3007, 3008, 3009, 3010],\n",
       " 1012: [3011, 3012, 3013, 3014, 3015],\n",
       " 1013: [3016, 3017],\n",
       " 1014: [3018, 3019],\n",
       " 1015: [3020],\n",
       " 1016: [3021, 3022, 3023, 3024, 3025, 3026],\n",
       " 1017: [3027, 3028, 3029, 3030, 3031],\n",
       " 1018: [3032, 3033, 3034, 3035, 3036],\n",
       " 1019: [3037, 3038, 3039, 3040, 3041, 3042, 3043],\n",
       " 1020: [3044, 3045, 3046],\n",
       " 1021: [3047, 3048],\n",
       " 1022: [3049, 3050, 3051, 3052],\n",
       " 1023: [3053, 3054, 3055, 3056],\n",
       " 1024: [3057],\n",
       " 1026: [3058],\n",
       " 1027: [3059],\n",
       " 1028: [3060],\n",
       " 1029: [3061, 3062, 3063, 3064],\n",
       " 1030: [3065, 3066, 3067],\n",
       " 1031: [3068, 3069, 3070, 3071],\n",
       " 1032: [3072, 3073, 3074, 3075, 3076],\n",
       " 1033: [3077],\n",
       " 1034: [3078, 3079],\n",
       " 1035: [3080, 3081],\n",
       " 1036: [3082, 3083, 3084],\n",
       " 1037: [3085, 3086],\n",
       " 1038: [3087, 3088, 3089],\n",
       " 1039: [3090, 3091, 3092, 3093, 3094, 3095],\n",
       " 1040: [3096, 3097],\n",
       " 1042: [3098, 3099, 3100, 3101],\n",
       " 1043: [3102],\n",
       " 1044: [3103, 3104, 3105],\n",
       " 1045: [3106, 3107],\n",
       " 1046: [3108, 3109, 3110],\n",
       " 1047: [3111, 3112, 3113, 3114],\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_per_acct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>tcode</th>\n",
       "      <th>datetime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dow</th>\n",
       "      <th>day</th>\n",
       "      <th>dtme</th>\n",
       "      <th>td</th>\n",
       "      <th>age</th>\n",
       "      <th>log_amount</th>\n",
       "      <th>log_amount_sc</th>\n",
       "      <th>td_sc</th>\n",
       "      <th>age_sc</th>\n",
       "      <th>tcode_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>CREDIT__CREDIT IN CASH__nan</td>\n",
       "      <td>1995-03-24</td>\n",
       "      <td>1995</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>3.000434</td>\n",
       "      <td>2.823750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>CREDIT__COLLECTION FROM ANOTHER BANK__nan</td>\n",
       "      <td>1995-04-13</td>\n",
       "      <td>1995</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>20.0</td>\n",
       "      <td>29</td>\n",
       "      <td>3.565848</td>\n",
       "      <td>3.355869</td>\n",
       "      <td>3.298201</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>12600.0</td>\n",
       "      <td>CREDIT__CREDIT IN CASH__nan</td>\n",
       "      <td>1995-04-23</td>\n",
       "      <td>1995</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29</td>\n",
       "      <td>4.100405</td>\n",
       "      <td>3.858949</td>\n",
       "      <td>1.649100</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>19.2</td>\n",
       "      <td>CREDIT__nan__INTEREST CREDITED</td>\n",
       "      <td>1995-04-30</td>\n",
       "      <td>1995</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.305351</td>\n",
       "      <td>1.228484</td>\n",
       "      <td>1.154370</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>CREDIT__COLLECTION FROM ANOTHER BANK__nan</td>\n",
       "      <td>1995-05-13</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>29</td>\n",
       "      <td>3.565848</td>\n",
       "      <td>3.355869</td>\n",
       "      <td>2.143831</td>\n",
       "      <td>1.745524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056315</th>\n",
       "      <td>11382</td>\n",
       "      <td>25600.0</td>\n",
       "      <td>DEBIT__CASH WITHDRAWAL__nan</td>\n",
       "      <td>1998-12-02</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46</td>\n",
       "      <td>4.408257</td>\n",
       "      <td>4.148672</td>\n",
       "      <td>0.329820</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056316</th>\n",
       "      <td>11382</td>\n",
       "      <td>46248.0</td>\n",
       "      <td>CREDIT__COLLECTION FROM ANOTHER BANK__nan</td>\n",
       "      <td>1998-12-10</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>8.0</td>\n",
       "      <td>46</td>\n",
       "      <td>4.665102</td>\n",
       "      <td>4.390393</td>\n",
       "      <td>1.319280</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056317</th>\n",
       "      <td>11382</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>DEBIT__CASH WITHDRAWAL__nan</td>\n",
       "      <td>1998-12-25</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.799409</td>\n",
       "      <td>3.575677</td>\n",
       "      <td>2.473651</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056318</th>\n",
       "      <td>11382</td>\n",
       "      <td>311.3</td>\n",
       "      <td>CREDIT__nan__INTEREST CREDITED</td>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.494572</td>\n",
       "      <td>2.347677</td>\n",
       "      <td>0.989460</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056319</th>\n",
       "      <td>11382</td>\n",
       "      <td>301.1</td>\n",
       "      <td>CREDIT__nan__INTEREST CREDITED</td>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.480151</td>\n",
       "      <td>2.334105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.768763</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056320 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         account_id   amount                                      tcode  \\\n",
       "0                 1   1000.0                CREDIT__CREDIT IN CASH__nan   \n",
       "1                 1   3679.0  CREDIT__COLLECTION FROM ANOTHER BANK__nan   \n",
       "2                 1  12600.0                CREDIT__CREDIT IN CASH__nan   \n",
       "3                 1     19.2             CREDIT__nan__INTEREST CREDITED   \n",
       "4                 1   3679.0  CREDIT__COLLECTION FROM ANOTHER BANK__nan   \n",
       "...             ...      ...                                        ...   \n",
       "1056315       11382  25600.0                DEBIT__CASH WITHDRAWAL__nan   \n",
       "1056316       11382  46248.0  CREDIT__COLLECTION FROM ANOTHER BANK__nan   \n",
       "1056317       11382   6300.0                DEBIT__CASH WITHDRAWAL__nan   \n",
       "1056318       11382    311.3             CREDIT__nan__INTEREST CREDITED   \n",
       "1056319       11382    301.1             CREDIT__nan__INTEREST CREDITED   \n",
       "\n",
       "          datetime  year  month  dow  day  dtme    td  age  log_amount  \\\n",
       "0       1995-03-24  1995      3    4   24     7   0.0   29    3.000434   \n",
       "1       1995-04-13  1995      4    3   13    17  20.0   29    3.565848   \n",
       "2       1995-04-23  1995      4    6   23     7  10.0   29    4.100405   \n",
       "3       1995-04-30  1995      4    6   30     0   7.0   29    1.305351   \n",
       "4       1995-05-13  1995      5    5   13    18  13.0   29    3.565848   \n",
       "...            ...   ...    ...  ...  ...   ...   ...  ...         ...   \n",
       "1056315 1998-12-02  1998     12    2    2    29   2.0   46    4.408257   \n",
       "1056316 1998-12-10  1998     12    3   10    21   8.0   46    4.665102   \n",
       "1056317 1998-12-25  1998     12    4   25     6  15.0   46    3.799409   \n",
       "1056318 1998-12-31  1998     12    3   31     0   6.0   46    2.494572   \n",
       "1056319 1998-12-31  1998     12    3   31     0   0.0   46    2.480151   \n",
       "\n",
       "         log_amount_sc     td_sc    age_sc  tcode_num  \n",
       "0             2.823750  0.000000  1.745524          0  \n",
       "1             3.355869  3.298201  1.745524          1  \n",
       "2             3.858949  1.649100  1.745524          0  \n",
       "3             1.228484  1.154370  1.745524          2  \n",
       "4             3.355869  2.143831  1.745524          1  \n",
       "...                ...       ...       ...        ...  \n",
       "1056315       4.148672  0.329820  2.768763          3  \n",
       "1056316       4.390393  1.319280  2.768763          1  \n",
       "1056317       3.575677  2.473651  2.768763          3  \n",
       "1056318       2.347677  0.989460  2.768763          2  \n",
       "1056319       2.334105  0.000000  2.768763          2  \n",
       "\n",
       "[1056320 rows x 16 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and create tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_cv, inds_tr, inds_cv, targ_tr, targ_cv = train_test_split(inp_tensor, np.arange(n_seqs), tar_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11483, 81, 26),\n",
       " (2871, 81, 26),\n",
       " array([  218, 13377,  3013, ...,  5220, 11025,  8408]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.shape, x_cv.shape, inds_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tr = tf.data.Dataset.from_tensor_slices((x_tr.astype(np.float32), targ_tr.astype(np.float32)))\n",
    "ds_cv = tf.data.Dataset.from_tensor_slices((x_cv.astype(np.float32), targ_cv.astype(np.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((81, 26), (80, 7)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 81, 26), (None, 80, 7)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_batches(ds, buffer_size, batch_size):\n",
    "    return ds.cache().shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "BUFFER_SIZE = ds_tr.cardinality().numpy()\n",
    "bs = 64    #batch size\n",
    "train_batches = make_batches(ds_tr, BUFFER_SIZE, bs)\n",
    "train_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features of class Transformer\n",
    "ACTIVATIONS = {\n",
    "    \"td_sc\": \"relu\",\n",
    "    \"log_amount_sc\": \"relu\"\n",
    "}\n",
    "config = {}\n",
    "config[\"ORDER\"] = DATA_KEY_ORDER\n",
    "config[\"FIELD_STARTS_IN\"] = FIELD_STARTS_IN\n",
    "config[\"FIELD_DIMS_IN\"] = FIELD_DIMS_IN\n",
    "config[\"FIELD_STARTS_NET\"] = FIELD_STARTS_NET\n",
    "config[\"FIELD_DIMS_NET\"] = FIELD_DIMS_NET\n",
    "\n",
    "config[\"ACTIVATIONS\"] = ACTIVATIONS\n",
    "\n",
    "num_layers_enc = None  # no encoder\n",
    "num_layers_dec = 4\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "dff = d_model\n",
    "maximum_position_encoding = 256\n",
    "net_info = None\n",
    "inp_dim = n_feat_inp\n",
    "final_dim = None\n",
    "rate=0.1\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "EARLY_STOP = 2\n",
    "EPOCHS = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create only one sample of inp and tar to trace one iteration of loop\n",
    "j = 0\n",
    "for (batch_no, (inp, tar)) in enumerate(train_batches):\n",
    "    if j == 0:\n",
    "       break\n",
    "\n",
    "batch_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 80, 7), dtype=float32, numpy=\n",
       "array([[[ 4.        ,  5.        ,  8.        , ...,  0.        ,\n",
       "          3.463111  ,  1.1228663 ],\n",
       "        [ 2.        ,  5.        ,  8.        , ...,  0.        ,\n",
       "          0.        ,  1.7245997 ],\n",
       "        [ 5.        ,  4.        ,  9.        , ..., 24.        ,\n",
       "          0.98946023,  3.1327682 ],\n",
       "        ...,\n",
       "        [ 3.        ,  3.        ,  0.        , ..., 20.        ,\n",
       "          0.16491003,  2.6722724 ],\n",
       "        [ 3.        ,  3.        ,  0.        , ..., 13.        ,\n",
       "          1.1543703 ,  2.972663  ],\n",
       "        [ 3.        ,  5.        ,  0.        , ..., 11.        ,\n",
       "          0.32982007,  2.8772542 ]],\n",
       "\n",
       "       [[ 3.        ,  4.        ,  7.        , ...,  6.        ,\n",
       "          2.8034706 ,  3.9813588 ],\n",
       "        [ 4.        ,  3.        ,  7.        , ...,  0.        ,\n",
       "          0.98946023,  1.1228663 ],\n",
       "        [ 2.        ,  3.        ,  7.        , ...,  0.        ,\n",
       "          0.        ,  2.1574826 ],\n",
       "        ...,\n",
       "        [ 2.        ,  3.        ,  0.        , ...,  0.        ,\n",
       "          4.1227508 ,  2.3614457 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 4.        ,  2.        , 11.        , ...,  0.        ,\n",
       "          0.        ,  1.1228663 ],\n",
       "        [ 3.        ,  3.        ,  0.        , ..., 23.        ,\n",
       "          1.3192803 ,  3.5756774 ],\n",
       "        [ 3.        ,  6.        ,  0.        , ..., 20.        ,\n",
       "          0.49473011,  3.2586522 ],\n",
       "        ...,\n",
       "        [ 6.        ,  3.        , 10.        , ..., 19.        ,\n",
       "          0.65964013,  3.1222825 ],\n",
       "        [ 0.        ,  5.        , 10.        , ..., 17.        ,\n",
       "          0.32982007,  4.385709  ],\n",
       "        [ 3.        ,  3.        , 10.        , ...,  5.        ,\n",
       "          1.9789205 ,  4.294767  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.        ,  2.        ,  8.        , ..., 18.        ,\n",
       "          0.32982007,  2.4498556 ],\n",
       "        [ 4.        ,  6.        ,  8.        , ...,  0.        ,\n",
       "          2.9683807 ,  1.1228663 ],\n",
       "        [ 2.        ,  6.        ,  8.        , ...,  0.        ,\n",
       "          0.        ,  1.7281903 ],\n",
       "        ...,\n",
       "        [ 1.        ,  0.        , 11.        , ..., 21.        ,\n",
       "          1.4841903 ,  3.3169806 ],\n",
       "        [ 5.        ,  2.        , 11.        , ..., 19.        ,\n",
       "          0.32982007,  3.1513197 ],\n",
       "        [ 4.        ,  0.        , 11.        , ...,  0.        ,\n",
       "          3.1332908 ,  1.1228663 ]],\n",
       "\n",
       "       [[ 5.        ,  5.        ,  9.        , ..., 18.        ,\n",
       "          0.        ,  2.6693044 ],\n",
       "        [ 7.        ,  6.        ,  9.        , ..., 17.        ,\n",
       "          0.16491003,  2.9723792 ],\n",
       "        [ 4.        ,  2.        ,  9.        , ...,  0.        ,\n",
       "          2.8034706 ,  1.1228663 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 7.        ,  6.        ,  9.        , ..., 23.        ,\n",
       "          0.16491003,  3.4232924 ],\n",
       "        [ 3.        ,  6.        ,  9.        , ..., 23.        ,\n",
       "          0.        ,  3.379706  ],\n",
       "        [ 8.        ,  5.        ,  9.        , ..., 17.        ,\n",
       "          0.98946023,  3.6159809 ],\n",
       "        ...,\n",
       "        [ 0.        ,  6.        ,  5.        , ..., 14.        ,\n",
       "          0.65964013,  3.419603  ],\n",
       "        [ 2.        ,  6.        ,  5.        , ...,  0.        ,\n",
       "          2.3087406 ,  1.8982583 ],\n",
       "        [ 4.        ,  6.        ,  5.        , ...,  0.        ,\n",
       "          0.        ,  1.1228663 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len), a triangle matrix, upper are ones and, lower and main diagonal are zeroes\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(tf.reduce_sum(seq, axis=2), 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_masks(tar):\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return combined_mask, dec_target_padding_mask    # combined_mask : (batch_size, 1, seq_len, seq_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_mask : (batch_size, 1, seq_len, seq_len)\n",
    "# dec_padding_mask : (batch_size, 1, 1, seq_len)\n",
    "combined_mask, dec_padding_mask = create_masks(tar)   #tar (batch_size, seq_len, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    #pos (256,1), 256=maximum_position_encoding\n",
    "    # i (1, 128), 128=d_model(embedding dimension)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))    #angle_rates(1,128)\n",
    "    return pos * angle_rates     #(256,128)\n",
    "    \n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],    #(256,1)\n",
    "                          np.arange(d_model)[np.newaxis, :],       #(1, 128)\n",
    "                          d_model)\n",
    "    #angle_rads (256, 128)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]    #(1,256,128)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 80, 128), dtype=float32, numpy=\n",
       "array([[[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
       "          1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
       "        [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
       "          1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
       "        [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
       "          9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
       "        ...,\n",
       "        [ 9.9952018e-01, -3.0975033e-02, -6.4864838e-01, ...,\n",
       "          9.9994731e-01,  8.8917045e-03,  9.9996048e-01],\n",
       "        [ 5.1397848e-01, -8.5780311e-01, -9.9999952e-01, ...,\n",
       "          9.9994588e-01,  9.0071773e-03,  9.9995941e-01],\n",
       "        [-4.4411266e-01, -8.9597094e-01, -6.4716274e-01, ...,\n",
       "          9.9994451e-01,  9.1226511e-03,  9.9995840e-01]]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positional_encoding(maximum_position_encoding = 256, d_model = 128)\n",
    "pos_encoding = positional_encoding(maximum_position_encoding, d_model)   #(1,256,128)\n",
    "pos_encoding[:, :max_seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 80, 26), dtype=float32, numpy=\n",
       "array([[[25.        , 25.        , 25.        , ..., 25.        ,\n",
       "         25.        , 25.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          3.463111  ,  1.1228663 ],\n",
       "        [ 0.        ,  0.        ,  1.        , ...,  1.        ,\n",
       "          0.        ,  1.7245997 ],\n",
       "        ...,\n",
       "        [ 0.        ,  1.        ,  0.        , ..., -0.44039416,\n",
       "          0.65964013,  3.29849   ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.61210597,\n",
       "          0.16491003,  2.6722724 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.8743466 ,\n",
       "          1.1543703 ,  2.972663  ]],\n",
       "\n",
       "       [[48.        , 48.        , 48.        , ..., 48.        ,\n",
       "         48.        , 48.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.34730524,\n",
       "          2.8034706 ,  3.9813588 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          0.98946023,  1.1228663 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.34730524,\n",
       "          0.16491003,  2.8700266 ],\n",
       "        [ 0.        ,  0.        ,  1.        , ...,  1.        ,\n",
       "          4.1227508 ,  2.3614457 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[28.        , 28.        , 28.        , ..., 28.        ,\n",
       "         28.        , 28.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          0.        ,  1.1228663 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.05064917,\n",
       "          1.3192803 ,  3.5756774 ],\n",
       "        ...,\n",
       "        [ 1.        ,  0.        ,  0.        , ..., -0.05064917,\n",
       "          0.8245502 ,  4.19072   ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.7587581 ,\n",
       "          0.65964013,  3.1222825 ],\n",
       "        [ 1.        ,  0.        ,  0.        , ..., -0.95413923,\n",
       "          0.32982007,  4.385709  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[45.        , 45.        , 45.        , ..., 45.        ,\n",
       "         45.        , 45.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.8743466 ,\n",
       "          0.32982007,  2.4498556 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          2.9683807 ,  1.1228663 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  1.        , ...,  1.        ,\n",
       "          0.        ,  1.717934  ],\n",
       "        [ 0.        ,  1.        ,  0.        , ..., -0.44039416,\n",
       "          1.4841903 ,  3.3169806 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.7587581 ,\n",
       "          0.32982007,  3.1513197 ]],\n",
       "\n",
       "       [[28.        , 28.        , 28.        , ..., 28.        ,\n",
       "         28.        , 28.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.8743466 ,\n",
       "          0.        ,  2.6693044 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.95413923,\n",
       "          0.16491003,  2.9723792 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[38.        , 38.        , 38.        , ..., 38.        ,\n",
       "         38.        , 38.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.05064917,\n",
       "          0.16491003,  3.4232924 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.05064917,\n",
       "          0.        ,  3.379706  ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.8743466 ,\n",
       "          0.98946023,  3.6159809 ],\n",
       "        [ 1.        ,  0.        ,  0.        , ..., -0.95413923,\n",
       "          0.65964013,  3.419603  ],\n",
       "        [ 0.        ,  0.        ,  1.        , ...,  1.        ,\n",
       "          2.3087406 ,  1.8982583 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 80, 26), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          3.463111  ,  1.1228663 ],\n",
       "        [ 0.        ,  0.        ,  1.        , ...,  1.        ,\n",
       "          0.        ,  1.7245997 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.15142778,\n",
       "          0.98946023,  3.1327682 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.61210597,\n",
       "          0.16491003,  2.6722724 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.8743466 ,\n",
       "          1.1543703 ,  2.972663  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.61210597,\n",
       "          0.32982007,  2.8772542 ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.34730524,\n",
       "          2.8034706 ,  3.9813588 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          0.98946023,  1.1228663 ],\n",
       "        [ 0.        ,  0.        ,  1.        , ...,  1.        ,\n",
       "          0.        ,  2.1574826 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  1.        , ...,  1.        ,\n",
       "          4.1227508 ,  2.3614457 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          0.        ,  1.1228663 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.05064917,\n",
       "          1.3192803 ,  3.5756774 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.61210597,\n",
       "          0.49473011,  3.2586522 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.7587581 ,\n",
       "          0.65964013,  3.1222825 ],\n",
       "        [ 1.        ,  0.        ,  0.        , ..., -0.95413923,\n",
       "          0.32982007,  4.385709  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.528964  ,\n",
       "          1.9789205 ,  4.294767  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ..., -0.8743466 ,\n",
       "          0.32982007,  2.4498556 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          2.9683807 ,  1.1228663 ],\n",
       "        [ 0.        ,  0.        ,  1.        , ...,  1.        ,\n",
       "          0.        ,  1.7281903 ],\n",
       "        ...,\n",
       "        [ 0.        ,  1.        ,  0.        , ..., -0.44039416,\n",
       "          1.4841903 ,  3.3169806 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.7587581 ,\n",
       "          0.32982007,  3.1513197 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          3.1332908 ,  1.1228663 ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ..., -0.8743466 ,\n",
       "          0.        ,  2.6693044 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.95413923,\n",
       "          0.16491003,  2.9723792 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          2.8034706 ,  1.1228663 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ..., -0.05064917,\n",
       "          0.16491003,  3.4232924 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.05064917,\n",
       "          0.        ,  3.379706  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.95413923,\n",
       "          0.98946023,  3.6159809 ],\n",
       "        ...,\n",
       "        [ 1.        ,  0.        ,  0.        , ..., -0.95413923,\n",
       "          0.65964013,  3.419603  ],\n",
       "        [ 0.        ,  0.        ,  1.        , ...,  1.        ,\n",
       "          2.3087406 ,  1.8982583 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          0.        ,  1.1228663 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_inp = inp[:, :-1] # predict next from this   (64, 80, 26)   , inp:(64, 81, 26)\n",
    "tar_out = inp[:, 1:]  #(64, 80, 26)\n",
    "tar_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.decoder = Decoder(num_layers_dec, d_model, num_heads, dff, maximum_position_encoding, inp_dim,rate)\n",
    "#dec_output, attention_weights = self.decoder(tar_inp, training, look_ahead_mask, dec_padding_mask)\n",
    "#seq_len = 80\n",
    "#input layer\n",
    "inp = tf.keras.layers.Input((None, inp_dim))           #(batch_size, seq_len, inp_feature)\n",
    "x = tf.keras.layers.Dense(dff, activation='relu')(inp)       #(batch_size, seq_len, dff=128)\n",
    "x = tf.keras.layers.Dense(d_model)(x)                        # (batch_size, seq_len, d_model=128)\n",
    "#seq_len = tf.shape(x)[1]\n",
    "#seq_len = 80\n",
    "attention_weights_dic = {}\n",
    "seq_len = tf.shape(x)[1]\n",
    "x += pos_encoding[:, :seq_len, :]     #x is the output of Input layer\n",
    "x = tf.keras.layers.Dropout(rate)(x, training=True)\n",
    "#Decoder Layer\n",
    "#Decoder Block1\n",
    "## MultiHeadAttention\n",
    "assert d_model % num_heads == 0\n",
    "depth = d_model // num_heads\n",
    "q = tf.keras.layers.Dense(d_model)(x)    #Wq\n",
    "k = tf.keras.layers.Dense(d_model)(x)    #Wk\n",
    "v = tf.keras.layers.Dense(d_model)(x)    #Wv\n",
    "\n",
    "#batch_size = bs\n",
    "batch_size = tf.shape(q)[0]\n",
    "q = tf.reshape(q, (batch_size, -1, num_heads, depth))\n",
    "q = tf.transpose(q, perm=[0,2,1,3])                    #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "k = tf.reshape(k, (batch_size, -1, num_heads, depth))\n",
    "k = tf.transpose(k, perm=[0,2,1,3])                     #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "v = tf.reshape(v, (batch_size, -1, num_heads, depth))\n",
    "v = tf.transpose(v, perm=[0,2,1,3])                      #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "matmul_qk = tf.matmul(q, k, transpose_b=True)     #scores  (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "# scale scores(matmul_qk)\n",
    "dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)     #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "# add the mask to the scaled tensor\n",
    "scaled_attention_logits += (combined_mask * -1e9)           #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "# softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "scaled_attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "concat_attention = tf.reshape(scaled_attention, (batch_size, -1, d_model))  # (batch_size, seq_len_q, d_model)\n",
    "output = tf.keras.layers.Dense(d_model)(concat_attention)\n",
    "\n",
    "\n",
    "attn_weights_block1 = attention_weights\n",
    "attn1 = output\n",
    "\n",
    "attn1 = tf.keras.layers.Dropout(rate)(attn1, training=True)\n",
    "out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn1 + x)    #Add & Norm\n",
    "\n",
    "#point-wise feed forward network\n",
    "ffn_output = tf.keras.layers.Dense(dff, activation='relu')(out1)  # (batch_size, seq_len, dff)\n",
    "ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)  # (batch_size, seq_len, d_model)\n",
    "ffn_output = tf.keras.layers.Dropout(rate)(ffn_output, training=True)\n",
    "\n",
    "out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_output+out1)   # (batch_size, seq_len, d_model)\n",
    "\n",
    "i = 0\n",
    "attention_weights_dic['decoder_layer{}_block1'.format(i+1)] = attn_weights_block1\n",
    "\n",
    "\n",
    "#Decoder Block2\n",
    "x = out3\n",
    "q = tf.keras.layers.Dense(d_model)(x)    #Wq\n",
    "k = tf.keras.layers.Dense(d_model)(x)    #Wk\n",
    "v = tf.keras.layers.Dense(d_model)(x)    #Wv\n",
    "\n",
    "#batch_size = bs\n",
    "batch_size = tf.shape(q)[0]\n",
    "q = tf.reshape(q, (batch_size, -1, num_heads, depth))\n",
    "q = tf.transpose(q, perm=[0,2,1,3])                    #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "k = tf.reshape(k, (batch_size, -1, num_heads, depth))\n",
    "k = tf.transpose(k, perm=[0,2,1,3])                     #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "v = tf.reshape(v, (batch_size, -1, num_heads, depth))\n",
    "v = tf.transpose(v, perm=[0,2,1,3])                      #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "matmul_qk = tf.matmul(q, k, transpose_b=True)     #scores  (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "# scale scores(matmul_qk)\n",
    "dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)     #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "# add the mask to the scaled tensor\n",
    "scaled_attention_logits += (combined_mask * -1e9)           #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "# softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "scaled_attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "concat_attention = tf.reshape(scaled_attention, (batch_size, -1, d_model))  # (batch_size, seq_len_q, d_model)\n",
    "output = tf.keras.layers.Dense(d_model)(concat_attention)\n",
    "\n",
    "\n",
    "attn_weights_block2 = attention_weights\n",
    "attn1 = output\n",
    "\n",
    "attn1 = tf.keras.layers.Dropout(rate)(attn1, training=True)\n",
    "out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn1 + x)    #Add & Norm\n",
    "\n",
    "#point-wise feed forward network\n",
    "ffn_output = tf.keras.layers.Dense(dff, activation='relu')(out1)  # (batch_size, seq_len, dff)\n",
    "ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)  # (batch_size, seq_len, d_model)\n",
    "ffn_output = tf.keras.layers.Dropout(rate)(ffn_output, training=True)\n",
    "\n",
    "out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_output+out1)   # (batch_size, seq_len, d_model)\n",
    "\n",
    "i = 1\n",
    "attention_weights_dic['decoder_layer{}_block1'.format(i+1)] = attn_weights_block2\n",
    "\n",
    "#Decoder Block3\n",
    "x = out3\n",
    "q = tf.keras.layers.Dense(d_model)(x)    #Wq\n",
    "k = tf.keras.layers.Dense(d_model)(x)    #Wk\n",
    "v = tf.keras.layers.Dense(d_model)(x)    #Wv\n",
    "\n",
    "#batch_size = bs\n",
    "batch_size = tf.shape(q)[0]\n",
    "q = tf.reshape(q, (batch_size, -1, num_heads, depth))\n",
    "q = tf.transpose(q, perm=[0,2,1,3])                    #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "k = tf.reshape(k, (batch_size, -1, num_heads, depth))\n",
    "k = tf.transpose(k, perm=[0,2,1,3])                     #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "v = tf.reshape(v, (batch_size, -1, num_heads, depth))\n",
    "v = tf.transpose(v, perm=[0,2,1,3])                      #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "matmul_qk = tf.matmul(q, k, transpose_b=True)     #scores  (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "# scale scores(matmul_qk)\n",
    "dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)     #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "# add the mask to the scaled tensor\n",
    "scaled_attention_logits += (combined_mask * -1e9)           #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "# softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "scaled_attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "concat_attention = tf.reshape(scaled_attention, (batch_size, -1, d_model))  # (batch_size, seq_len_q, d_model)\n",
    "output = tf.keras.layers.Dense(d_model)(concat_attention)\n",
    "\n",
    "\n",
    "attn_weights_block3 = attention_weights\n",
    "attn1 = output\n",
    "\n",
    "attn1 = tf.keras.layers.Dropout(rate)(attn1, training=True)\n",
    "out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn1 + x)    #Add & Norm\n",
    "\n",
    "#point-wise feed forward network\n",
    "ffn_output = tf.keras.layers.Dense(dff, activation='relu')(out1)  # (batch_size, seq_len, dff)\n",
    "ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)  # (batch_size, seq_len, d_model)\n",
    "ffn_output = tf.keras.layers.Dropout(rate)(ffn_output, training=True)\n",
    "\n",
    "out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_output+out1)   # (batch_size, seq_len, d_model)\n",
    "\n",
    "i = 2\n",
    "attention_weights_dic['decoder_layer{}_block1'.format(i+1)] = attn_weights_block3\n",
    "\n",
    "#Decoder Block4\n",
    "x = out3\n",
    "q = tf.keras.layers.Dense(d_model)(x)    #Wq\n",
    "k = tf.keras.layers.Dense(d_model)(x)    #Wk\n",
    "v = tf.keras.layers.Dense(d_model)(x)    #Wv\n",
    "\n",
    "#batch_size = bs\n",
    "batch_size = tf.shape(q)[0]\n",
    "q = tf.reshape(q, (batch_size, -1, num_heads, depth))\n",
    "q = tf.transpose(q, perm=[0,2,1,3])                    #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "k = tf.reshape(k, (batch_size, -1, num_heads, depth))\n",
    "k = tf.transpose(k, perm=[0,2,1,3])                     #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "v = tf.reshape(v, (batch_size, -1, num_heads, depth))\n",
    "v = tf.transpose(v, perm=[0,2,1,3])                      #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "matmul_qk = tf.matmul(q, k, transpose_b=True)     #scores  (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "# scale scores(matmul_qk)\n",
    "dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)     #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "# add the mask to the scaled tensor\n",
    "scaled_attention_logits += (combined_mask * -1e9)           #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "# softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "scaled_attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "concat_attention = tf.reshape(scaled_attention, (batch_size, -1, d_model))  # (batch_size, seq_len_q, d_model)\n",
    "output = tf.keras.layers.Dense(d_model)(concat_attention)\n",
    "\n",
    "\n",
    "attn_weights_block4 = attention_weights\n",
    "attn1 = output\n",
    "\n",
    "attn1 = tf.keras.layers.Dropout(rate)(attn1, training=True)\n",
    "out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn1 + x)    #Add & Norm\n",
    "\n",
    "#point-wise feed forward network\n",
    "ffn_output = tf.keras.layers.Dense(dff, activation='relu')(out1)  # (batch_size, seq_len, dff)\n",
    "ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)  # (batch_size, seq_len, d_model)\n",
    "ffn_output = tf.keras.layers.Dropout(rate)(ffn_output, training=True)\n",
    "\n",
    "out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_output+out1)   # (batch_size, seq_len, d_model)\n",
    "\n",
    "i = 3\n",
    "attention_weights_dic['decoder_layer{}_block1'.format(i+1)] = attn_weights_block4\n",
    "\n",
    "dec_output = out3\n",
    "\n",
    "final_output = tf.keras.layers.Dense(d_model, activation=None)(dec_output)\n",
    "model_pred = tf.keras.models.Model(inp , final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'td_sc': 'relu', 'log_amount_sc': 'relu'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTIVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['tcode_num', 'dow', 'month', 'day', 'dtme', 'td_sc', 'log_amount_sc'],\n",
       " {'tcode_num': 16,\n",
       "  'dow': 2,\n",
       "  'month': 2,\n",
       "  'day': 2,\n",
       "  'dtme': 2,\n",
       "  'td_sc': 1,\n",
       "  'log_amount_sc': 1})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_KEY_ORDER, FIELD_DIMS_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "preds = {}\n",
    "\n",
    "for name, dim in FIELD_DIMS_NET.items():\n",
    "    acti = ACTIVATIONS.get(name, None)\n",
    "    layers_dict[name] = tf.keras.layers.Dense(dim, activation=acti)\n",
    "final_output2 = final_output\n",
    "for net_name in DATA_KEY_ORDER:\n",
    "    #print(\"Running net\", net_name)\n",
    "    pred = layers_dict[net_name](final_output2)\n",
    "    #print(\"pred shape\", pred.shape)\n",
    "    preds[net_name] = pred\n",
    "    st = FIELD_STARTS_IN[net_name]\n",
    "    end = st + FIELD_DIMS_IN[net_name]\n",
    "    to_add = tar_out[:, :, st: end]\n",
    "    #print(\"Start and end\", st, end)\n",
    "            \n",
    "    final_output2 = tf.concat([final_output2, to_add], axis=-1)\n",
    "    #print(\"Final output shape after\",net_name, \"is\", final_output.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': <KerasTensor: shape=(None, None, 16) dtype=float32 (created by layer 'dense_27')>,\n",
       " 'dow': <KerasTensor: shape=(64, 80, 7) dtype=float32 (created by layer 'dense_28')>,\n",
       " 'month': <KerasTensor: shape=(64, 80, 12) dtype=float32 (created by layer 'dense_29')>,\n",
       " 'day': <KerasTensor: shape=(64, 80, 31) dtype=float32 (created by layer 'dense_30')>,\n",
       " 'dtme': <KerasTensor: shape=(64, 80, 31) dtype=float32 (created by layer 'dense_31')>,\n",
       " 'td_sc': <KerasTensor: shape=(64, 80, 2) dtype=float32 (created by layer 'dense_32')>,\n",
       " 'log_amount_sc': <KerasTensor: shape=(64, 80, 2) dtype=float32 (created by layer 'dense_33')>}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=inp, outputs=preds)   #this model is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 10:55:41.633314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    }
   ],
   "source": [
    "predictions = model(tar_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 80, 128), dtype=float32, numpy=\n",
       "array([[[ 2.47508794e-01,  2.31427848e-01,  6.85405135e-01, ...,\n",
       "         -9.50253308e-01,  1.94203764e-01,  3.38773757e-01],\n",
       "        [-5.24850368e-01, -3.96497637e-01, -4.07264791e-02, ...,\n",
       "         -1.21726763e+00,  1.14652133e+00, -5.22654772e-01],\n",
       "        [-1.32784799e-01, -1.06285310e+00,  8.40427697e-01, ...,\n",
       "         -2.10446879e-01,  5.04676163e-01,  1.98306844e-01],\n",
       "        ...,\n",
       "        [-1.26743758e+00, -1.70405477e-01,  8.92285645e-01, ...,\n",
       "         -1.52311623e+00,  1.05818474e+00,  5.15485816e-02],\n",
       "        [-8.73098612e-01, -9.36779007e-03,  5.91618657e-01, ...,\n",
       "         -9.17498767e-01,  1.25917017e+00, -3.97295803e-01],\n",
       "        [-1.08385921e+00, -1.88244104e-01, -4.61101979e-01, ...,\n",
       "         -1.43684518e+00,  1.10385203e+00, -1.38263345e-01]],\n",
       "\n",
       "       [[-1.36280549e+00, -7.83154786e-01,  8.76789689e-01, ...,\n",
       "         -9.98257637e-01,  3.76738042e-01,  4.07920986e-01],\n",
       "        [-6.01806879e-01, -2.85904169e-01,  1.17346954e+00, ...,\n",
       "         -1.68342829e+00,  1.03537619e+00,  4.99711931e-01],\n",
       "        [-1.06906545e+00, -2.36678883e-01,  1.83634102e-01, ...,\n",
       "         -9.42517221e-01,  2.18380645e-01,  6.47304356e-01],\n",
       "        ...,\n",
       "        [-9.63379443e-01,  6.73117861e-02,  5.60306013e-01, ...,\n",
       "         -1.04610157e+00,  1.75601542e-01,  3.02697450e-01],\n",
       "        [-1.16691756e+00, -2.71788402e-03, -1.30033314e-01, ...,\n",
       "         -1.14985096e+00,  4.13347938e-04, -7.27324188e-02],\n",
       "        [-3.68843943e-01,  2.72797436e-01,  3.06402713e-01, ...,\n",
       "         -1.42225564e+00,  3.52744721e-02, -2.91765898e-01]],\n",
       "\n",
       "       [[-7.19476581e-01, -3.52140993e-01,  1.50521636e+00, ...,\n",
       "         -1.10251856e+00, -1.11915618e-01,  8.31133649e-02],\n",
       "        [ 1.11326994e-02,  3.51944983e-01,  3.89567435e-01, ...,\n",
       "         -2.85903037e-01,  1.86487377e+00,  6.08067922e-02],\n",
       "        [-2.02007294e-01, -5.99344492e-01,  5.91365337e-01, ...,\n",
       "         -2.15998936e+00,  6.37979090e-01, -1.69855058e-01],\n",
       "        ...,\n",
       "        [-1.77421719e-01,  6.74130499e-01,  3.41917872e-01, ...,\n",
       "         -1.76459062e+00,  1.01370788e+00, -1.10055216e-01],\n",
       "        [-6.70778394e-01,  4.58975822e-01, -1.23342171e-01, ...,\n",
       "         -1.26158044e-01,  8.63432050e-01,  1.26959503e-01],\n",
       "        [-5.57003796e-01,  5.73631041e-02,  9.00512278e-01, ...,\n",
       "         -1.77811921e+00,  1.06983411e+00,  3.97422910e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-6.32606000e-02,  2.49761850e-01,  1.20896006e+00, ...,\n",
       "         -1.59938037e+00,  7.05723166e-01, -6.68897390e-01],\n",
       "        [-1.13551011e-02,  7.16689467e-01,  1.22408473e+00, ...,\n",
       "         -6.17138624e-01,  4.76981521e-01,  5.11163592e-01],\n",
       "        [ 2.02452809e-01,  3.81770223e-01,  6.54552519e-01, ...,\n",
       "         -1.07751346e+00,  8.42348278e-01,  1.08082139e+00],\n",
       "        ...,\n",
       "        [-1.70768094e+00,  2.59258449e-01,  4.78041321e-01, ...,\n",
       "         -1.43315208e+00,  1.34026647e+00, -7.15216696e-01],\n",
       "        [-9.08754542e-02,  6.01072907e-01,  6.58738554e-01, ...,\n",
       "         -8.82686913e-01,  1.06698263e+00,  4.98799145e-01],\n",
       "        [-4.72339272e-01,  1.17982674e+00,  9.90697086e-01, ...,\n",
       "         -8.45016718e-01,  4.76301253e-01,  9.61838424e-01]],\n",
       "\n",
       "       [[-6.11065626e-01, -2.32267275e-01,  1.36460900e+00, ...,\n",
       "         -8.32440555e-01, -9.53475460e-02, -2.76354164e-01],\n",
       "        [-4.10689414e-01,  7.41834223e-01,  6.97140217e-01, ...,\n",
       "         -4.72170949e-01,  5.53257585e-01, -1.15758181e-02],\n",
       "        [-9.37653929e-02,  2.39415956e-03,  8.70503247e-01, ...,\n",
       "         -5.29483080e-01, -3.79375517e-02,  1.58771917e-01],\n",
       "        ...,\n",
       "        [ 1.17446065e-01,  8.02004989e-03,  3.91442448e-01, ...,\n",
       "         -9.93717551e-01,  1.00954735e+00,  1.67655528e-01],\n",
       "        [-2.61125326e-01, -4.18604940e-01,  1.05524564e+00, ...,\n",
       "         -7.23793626e-01,  1.41101646e+00,  7.98414826e-01],\n",
       "        [-7.99884796e-01, -1.72725201e+00,  1.13806772e+00, ...,\n",
       "         -6.12544239e-01,  1.02584541e+00,  7.08878934e-02]],\n",
       "\n",
       "       [[-8.27639520e-01, -6.89059913e-01,  3.44053239e-01, ...,\n",
       "         -1.06298530e+00, -2.74548233e-01, -3.61568540e-01],\n",
       "        [-3.34863588e-02,  5.32395780e-01,  3.37796032e-01, ...,\n",
       "         -9.92633104e-01,  6.75669968e-01,  4.04716097e-02],\n",
       "        [-5.32543421e-01,  1.34804443e-01,  6.67907238e-01, ...,\n",
       "         -9.89208817e-01,  3.19464922e-01,  8.08685958e-01],\n",
       "        ...,\n",
       "        [-1.00900757e+00,  2.17367546e-03,  1.45754409e+00, ...,\n",
       "         -6.58547461e-01,  5.30352294e-01,  6.97065771e-01],\n",
       "        [-6.11371160e-01,  2.71666795e-01,  7.89447248e-01, ...,\n",
       "         -7.23831952e-01,  2.47993544e-01,  6.50653914e-02],\n",
       "        [-1.32568514e+00, -1.39702812e-01,  5.39679468e-01, ...,\n",
       "         -1.47506332e+00,  3.58515412e-01,  6.08367361e-02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred(tar_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': <tf.Tensor: shape=(64, 80, 16), dtype=float32, numpy=\n",
       " array([[[ 0.74753004,  0.12910928, -0.07689029, ...,  1.1557971 ,\n",
       "          -1.9513344 ,  1.469992  ],\n",
       "         [-0.04918024,  0.08208848,  0.88004893, ...,  0.01808494,\n",
       "          -2.1121235 ,  1.8796155 ],\n",
       "         [ 1.2310207 , -0.2557822 ,  0.82883936, ...,  0.25696054,\n",
       "          -1.6881715 ,  1.3324795 ],\n",
       "         ...,\n",
       "         [ 1.382555  ,  0.07207727,  0.6905937 , ..., -0.28981024,\n",
       "          -1.2691842 ,  1.5594574 ],\n",
       "         [ 2.6756737 , -0.0909652 ,  1.2991941 , ..., -0.469168  ,\n",
       "          -1.2052423 ,  1.3935015 ],\n",
       "         [ 2.2972739 , -0.6957778 , -0.478017  , ...,  1.8051989 ,\n",
       "          -2.094645  ,  2.6022766 ]],\n",
       " \n",
       "        [[ 1.2446796 ,  1.2328284 ,  1.0294368 , ..., -0.84119624,\n",
       "          -0.22446063,  2.0291536 ],\n",
       "         [ 1.2034487 , -1.1051345 ,  1.0070878 , ...,  0.6485173 ,\n",
       "          -1.2314957 ,  2.0270073 ],\n",
       "         [ 0.6963331 ,  0.29812345,  0.04276058, ...,  0.6608512 ,\n",
       "          -1.694806  ,  2.4581106 ],\n",
       "         ...,\n",
       "         [ 0.9108041 , -0.08216688,  1.2111036 , ...,  0.12623683,\n",
       "          -1.6609377 ,  2.0074205 ],\n",
       "         [ 2.5587366 ,  1.1602001 ,  0.25204194, ..., -1.114978  ,\n",
       "          -1.9485749 ,  2.451804  ],\n",
       "         [ 1.7887716 , -0.38279927,  0.56206226, ...,  0.5889751 ,\n",
       "          -1.4826664 ,  1.7872144 ]],\n",
       " \n",
       "        [[ 0.9723504 , -0.47022092,  1.2186425 , ...,  0.28986734,\n",
       "          -0.59426856,  2.3661647 ],\n",
       "         [ 1.5604613 , -0.6673979 ,  1.0201652 , ...,  0.08170433,\n",
       "          -1.8321699 ,  1.308921  ],\n",
       "         [ 0.7468988 ,  0.9843833 ,  2.0585625 , ...,  1.3704761 ,\n",
       "          -0.7079069 ,  1.3772826 ],\n",
       "         ...,\n",
       "         [ 0.2808547 ,  0.50414073,  1.2758884 , ...,  0.21987867,\n",
       "          -1.5046757 ,  0.7358484 ],\n",
       "         [ 1.2816712 ,  0.4719549 ,  0.7333822 , ...,  0.21296886,\n",
       "          -1.3395736 ,  0.8581282 ],\n",
       "         [ 1.4633073 ,  1.6375593 , -0.40014523, ...,  1.9316641 ,\n",
       "          -1.5447515 ,  1.1979704 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 1.1079739 ,  0.07750551,  0.5703377 , ...,  0.11113314,\n",
       "          -0.7229126 ,  1.7002747 ],\n",
       "         [ 2.4652085 , -0.05132879,  0.48983213, ...,  0.4407516 ,\n",
       "          -0.5426967 ,  1.4999249 ],\n",
       "         [ 1.1944798 , -0.20998876,  0.23809248, ...,  0.05832119,\n",
       "           0.11146989,  2.1973705 ],\n",
       "         ...,\n",
       "         [ 1.9104465 ,  1.0197089 , -0.3814266 , ...,  0.28082567,\n",
       "          -0.88780457,  1.1148621 ],\n",
       "         [ 0.5998569 ,  0.98361695,  0.5209585 , ..., -0.6498284 ,\n",
       "          -1.0579916 , -0.17173985],\n",
       "         [ 1.2354916 ,  0.40966946,  0.10388903, ...,  0.07397656,\n",
       "          -1.0182365 ,  0.1929267 ]],\n",
       " \n",
       "        [[ 1.6244894 , -0.11751032,  0.6152158 , ...,  0.33951217,\n",
       "          -0.7129263 ,  2.1780727 ],\n",
       "         [ 1.2004501 , -0.52551603,  0.5879169 , ..., -0.3778273 ,\n",
       "          -1.1514236 ,  1.9760058 ],\n",
       "         [ 1.6327097 ,  0.13796069,  0.5232774 , ...,  1.0244802 ,\n",
       "          -0.6099685 ,  1.275328  ],\n",
       "         ...,\n",
       "         [ 1.0989448 ,  0.17868267,  0.961459  , ..., -1.35279   ,\n",
       "          -0.5828892 ,  1.3723171 ],\n",
       "         [ 1.1371021 ,  0.6129895 ,  0.2611697 , ...,  0.1915307 ,\n",
       "           0.20866299,  0.19106479],\n",
       "         [ 1.8531369 ,  0.43737125, -0.5497833 , ...,  0.59597045,\n",
       "          -0.21956444,  0.597562  ]],\n",
       " \n",
       "        [[-0.36276892, -0.7653751 ,  1.4886667 , ..., -0.4435002 ,\n",
       "          -2.1081383 ,  2.4911084 ],\n",
       "         [ 0.65760946, -0.6075205 ,  0.19803979, ...,  0.97125685,\n",
       "          -1.7741703 ,  1.2811109 ],\n",
       "         [ 0.19764368, -0.2046168 ,  0.6359537 , ...,  0.702952  ,\n",
       "          -0.9688191 ,  1.7864565 ],\n",
       "         ...,\n",
       "         [ 1.6780587 ,  0.69352114,  0.5695352 , ..., -0.9655697 ,\n",
       "          -1.5945034 ,  0.38194   ],\n",
       "         [ 0.87878686, -0.14556459,  1.521403  , ..., -1.4467962 ,\n",
       "          -2.224594  ,  1.2301031 ],\n",
       "         [ 0.61681473, -0.40086523,  0.34668732, ..., -0.17635551,\n",
       "          -1.317143  ,  2.0271225 ]]], dtype=float32)>,\n",
       " 'dow': <tf.Tensor: shape=(64, 80, 7), dtype=float32, numpy=\n",
       " array([[[-5.4733828e-03,  3.6276624e-02, -6.9458455e-02, ...,\n",
       "          -1.0006259e-01,  2.6804909e-01, -8.1178141e-01],\n",
       "         [ 7.4772227e-01,  9.0451622e-01, -7.1102560e-01, ...,\n",
       "           5.7075667e-01, -9.7405326e-01,  2.6357388e-01],\n",
       "         [ 6.2107831e-01,  8.2797426e-01, -1.9602668e+00, ...,\n",
       "           2.4494994e-01, -8.0103981e-01, -2.5416365e-01],\n",
       "         ...,\n",
       "         [ 1.6745469e-01,  1.4215780e+00, -9.5662898e-01, ...,\n",
       "           6.8919241e-01, -7.2091353e-01,  1.9666359e-01],\n",
       "         [ 1.4907641e+00,  5.9337896e-01,  1.4111134e-01, ...,\n",
       "           1.2095485e+00, -1.4042594e+00,  2.6545280e-01],\n",
       "         [ 6.1724788e-01,  9.9759772e-03, -4.0370226e-04, ...,\n",
       "           4.9795142e-01, -8.7787747e-01,  1.1474277e+00]],\n",
       " \n",
       "        [[-1.9608761e-01,  1.6207001e+00, -9.5150799e-01, ...,\n",
       "           1.7444252e+00, -3.2922041e-01,  2.1560916e-01],\n",
       "         [ 3.4587643e-01,  2.5227535e-01, -6.0103363e-01, ...,\n",
       "           1.3449249e+00, -9.2147052e-01,  1.8750550e-01],\n",
       "         [ 3.1012553e-01,  1.2728293e+00, -4.6568212e-01, ...,\n",
       "           1.4578459e+00, -1.1990459e+00, -6.0625499e-01],\n",
       "         ...,\n",
       "         [ 1.6536336e+00,  6.4401817e-01, -4.9473837e-01, ...,\n",
       "           8.9777243e-01, -1.8875283e+00,  1.6070848e+00],\n",
       "         [ 1.6252320e+00,  6.9257730e-01, -8.6819196e-01, ...,\n",
       "           1.6809393e+00, -1.7019601e+00,  1.6597867e+00],\n",
       "         [ 1.1992720e+00,  1.0925980e+00, -7.7256429e-01, ...,\n",
       "           1.1862735e+00, -2.7180678e-01,  1.8182234e-01]],\n",
       " \n",
       "        [[ 4.8135242e-01,  1.4140158e+00, -3.9316297e-02, ...,\n",
       "           5.0319111e-01, -1.4249368e+00,  5.5695921e-02],\n",
       "         [ 1.9822004e-01,  2.0694733e+00,  2.6592016e-01, ...,\n",
       "           1.2070347e+00, -6.0949957e-01,  8.1547558e-01],\n",
       "         [ 1.3141371e+00,  1.1978595e+00,  2.8691033e-01, ...,\n",
       "           6.8632483e-01, -9.0741116e-01,  7.2995245e-01],\n",
       "         ...,\n",
       "         [ 9.3018234e-01,  3.7491745e-01, -8.8545167e-01, ...,\n",
       "           4.5207125e-01, -2.1811922e+00, -9.8095500e-01],\n",
       "         [ 7.8637707e-01,  1.2234497e+00, -1.2671757e+00, ...,\n",
       "           8.3301115e-01, -1.5680879e+00, -6.7187625e-01],\n",
       "         [ 1.0203294e+00,  1.3800908e+00, -3.2764858e-01, ...,\n",
       "           4.2224523e-01, -1.7886071e+00, -4.5722559e-01]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-3.5057977e-01,  4.6169543e-01, -9.2110819e-01, ...,\n",
       "           2.3962754e-01, -1.3698791e-01, -6.6516423e-01],\n",
       "         [ 1.9138256e-01,  1.2414293e+00, -4.9972975e-01, ...,\n",
       "          -4.6078831e-01,  5.9161645e-01, -2.4322224e-01],\n",
       "         [ 2.4663766e-01,  1.2097833e+00, -1.2890189e+00, ...,\n",
       "           6.9398582e-02, -6.7736948e-01,  9.7099513e-02],\n",
       "         ...,\n",
       "         [ 1.7977060e+00, -5.7994926e-01, -1.2107935e+00, ...,\n",
       "           3.0734617e-01, -6.0343397e-01,  7.5176430e-01],\n",
       "         [ 1.6332183e+00,  9.1162920e-03, -2.1589231e+00, ...,\n",
       "           1.1458923e+00, -1.2416778e+00,  1.7486748e+00],\n",
       "         [ 1.1712142e+00, -2.5907746e-01, -1.0232248e+00, ...,\n",
       "           4.7798985e-01, -1.0595355e+00,  6.4103204e-01]],\n",
       " \n",
       "        [[ 5.0409466e-01,  1.1397043e+00, -5.8279097e-02, ...,\n",
       "           5.5750006e-01, -1.1523817e+00,  8.8078070e-01],\n",
       "         [ 1.1617737e+00,  1.3213644e+00, -3.4533092e-01, ...,\n",
       "           4.4006318e-02, -1.4761252e+00,  5.3575850e-01],\n",
       "         [ 5.6379825e-01,  1.7400734e+00, -4.4955552e-01, ...,\n",
       "          -5.7378948e-02, -1.0887690e+00,  2.3975334e-01],\n",
       "         ...,\n",
       "         [ 1.2420501e+00,  4.4967443e-01, -9.7832823e-01, ...,\n",
       "           6.1644155e-01, -2.2853026e+00, -3.4797437e-02],\n",
       "         [-1.2936835e-01,  1.2734904e+00,  4.7490367e-01, ...,\n",
       "           7.3332000e-01, -1.0481235e+00, -2.3687954e-01],\n",
       "         [ 1.2064345e+00,  8.5034883e-01, -4.1994035e-01, ...,\n",
       "           1.3849462e+00, -1.2360023e+00,  5.8150768e-01]],\n",
       " \n",
       "        [[ 1.4654872e+00, -9.9139586e-02, -7.6640081e-01, ...,\n",
       "           5.6717563e-01, -6.4585900e-01,  2.2680712e+00],\n",
       "         [ 1.2560509e+00, -4.3888077e-02, -1.7599888e+00, ...,\n",
       "           1.9083521e-01, -1.3267610e+00,  1.0171164e+00],\n",
       "         [ 9.8828894e-01, -2.5578010e-01, -1.1856273e+00, ...,\n",
       "          -3.8296658e-01, -1.4186817e+00,  2.4943550e+00],\n",
       "         ...,\n",
       "         [ 8.6103201e-01,  1.0883389e+00, -1.1066836e+00, ...,\n",
       "           7.3708034e-01, -2.0408979e-01, -3.1063604e-01],\n",
       "         [ 1.8534925e+00,  1.5901079e+00, -1.5886782e+00, ...,\n",
       "           7.3812366e-01, -1.3054202e+00,  4.6468303e-02],\n",
       "         [ 6.6845518e-01,  1.2587733e+00,  2.4574690e-02, ...,\n",
       "           5.8377594e-02, -5.0774693e-01,  4.2771271e-01]]], dtype=float32)>,\n",
       " 'month': <tf.Tensor: shape=(64, 80, 12), dtype=float32, numpy=\n",
       " array([[[ 0.28888065,  0.27066988,  1.2985348 , ..., -0.7935351 ,\n",
       "           3.7027106 , -0.5209002 ],\n",
       "         [-0.64403796, -0.3013343 ,  1.5850009 , ..., -0.30742523,\n",
       "           2.5940006 , -0.11161853],\n",
       "         [-0.1266924 , -0.44798794,  0.09359346, ...,  0.10030046,\n",
       "           1.6824793 , -0.7224433 ],\n",
       "         ...,\n",
       "         [-0.9154714 , -0.55315757,  1.5746984 , ...,  0.67864287,\n",
       "           1.8593143 ,  1.0248486 ],\n",
       "         [ 0.8583748 , -1.5439125 ,  2.8331141 , ..., -0.07511204,\n",
       "           2.5103116 ,  1.3234013 ],\n",
       "         [-1.3194894 , -1.4601309 ,  1.5064038 , ...,  0.31444573,\n",
       "           3.07271   ,  0.07744069]],\n",
       " \n",
       "        [[-0.24035823, -0.9399692 ,  0.54190785, ...,  0.6863249 ,\n",
       "           3.3519876 ,  1.5562125 ],\n",
       "         [ 0.2809262 , -0.8388136 ,  0.1643289 , ...,  0.40022463,\n",
       "           3.086469  ,  0.51468706],\n",
       "         [ 0.20056508, -0.4545841 ,  1.1752253 , ...,  0.3221382 ,\n",
       "           2.653739  ,  0.5595663 ],\n",
       "         ...,\n",
       "         [-0.1724933 , -1.0691794 ,  0.38606715, ...,  1.4886627 ,\n",
       "           1.9306103 ,  0.4435847 ],\n",
       "         [-0.26770604, -1.1621046 ,  1.3141606 , ...,  0.61450607,\n",
       "           1.3927256 ,  1.8883369 ],\n",
       "         [-0.33133402, -0.9238755 ,  0.3791466 , ...,  1.6117724 ,\n",
       "           3.3824022 , -0.44133642]],\n",
       " \n",
       "        [[ 0.26015443, -0.22955103,  0.19437684, ..., -0.01726568,\n",
       "           2.6335442 , -0.69278306],\n",
       "         [-1.1843035 , -1.8370588 ,  0.51487005, ..., -0.22624208,\n",
       "           2.1379433 , -0.32975158],\n",
       "         [-1.2525413 , -1.9299314 , -0.02345224, ..., -0.9320911 ,\n",
       "           2.1197143 ,  0.68021077],\n",
       "         ...,\n",
       "         [-0.0581307 , -0.9562292 ,  0.4718771 , ...,  1.0818138 ,\n",
       "           0.86116654,  1.5381321 ],\n",
       "         [-0.29586205, -0.84727585,  0.4931317 , ...,  0.8499099 ,\n",
       "           1.0694238 ,  0.83983684],\n",
       "         [-0.41088906,  0.01870839,  1.2255199 , ...,  0.3498423 ,\n",
       "           1.94794   ,  0.9183572 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.18094279,  0.02436588,  0.27511054, ..., -0.79702073,\n",
       "           2.0852005 ,  0.58891726],\n",
       "         [ 0.47279724,  0.5484543 ,  1.5218972 , ...,  0.45406157,\n",
       "           1.2835406 , -0.3933075 ],\n",
       "         [-0.5616202 , -0.35608095,  0.6672292 , ...,  0.2383196 ,\n",
       "           1.549369  , -0.00592042],\n",
       "         ...,\n",
       "         [ 0.00999185, -0.8477164 ,  1.4073895 , ...,  0.9696466 ,\n",
       "           1.4332459 , -0.01121508],\n",
       "         [-1.934401  , -0.914114  ,  0.13409047, ...,  1.0981797 ,\n",
       "           1.471442  ,  0.24549577],\n",
       "         [-0.6611865 , -1.21586   ,  0.8716467 , ...,  0.781481  ,\n",
       "           2.1592813 , -0.28280896]],\n",
       " \n",
       "        [[-1.1728433 , -0.3625715 ,  0.34829012, ...,  1.3327539 ,\n",
       "           2.8297827 , -0.4259239 ],\n",
       "         [ 0.42217937, -0.35945594,  0.7759854 , ...,  1.1571517 ,\n",
       "           1.8040419 , -0.6214184 ],\n",
       "         [-0.4121607 , -0.89601296, -0.06431489, ...,  0.01153207,\n",
       "           3.3355222 , -0.9976936 ],\n",
       "         ...,\n",
       "         [-0.40674424, -1.2082517 ,  0.07117964, ...,  1.1800342 ,\n",
       "           1.4834225 ,  1.1483672 ],\n",
       "         [ 0.23352118,  0.05369029,  0.9183984 , ...,  0.6643081 ,\n",
       "           1.5755578 ,  0.5712962 ],\n",
       "         [-1.0597485 , -0.63423824,  0.2590187 , ...,  0.38445112,\n",
       "           1.1940894 ,  0.65907973]],\n",
       " \n",
       "        [[-0.38147524, -0.0114575 ,  0.22023256, ..., -0.01145955,\n",
       "           1.2102811 ,  0.19716342],\n",
       "         [-0.60910094, -1.209491  ,  1.3100977 , ...,  0.34581974,\n",
       "           1.6930931 ,  0.3713135 ],\n",
       "         [-0.74196255,  0.06721713,  1.6095884 , ..., -1.2806343 ,\n",
       "           1.6376178 ,  1.2913432 ],\n",
       "         ...,\n",
       "         [-0.4919881 , -0.6066339 ,  0.5830376 , ...,  0.37674856,\n",
       "           1.8942343 , -0.6845372 ],\n",
       "         [-0.55448943, -1.7131016 , -0.3463695 , ...,  0.22139737,\n",
       "          -0.06408313,  1.1219814 ],\n",
       "         [-0.5596803 , -0.666922  ,  1.3273743 , ...,  0.47530568,\n",
       "           1.7048714 ,  0.30710995]]], dtype=float32)>,\n",
       " 'day': <tf.Tensor: shape=(64, 80, 31), dtype=float32, numpy=\n",
       " array([[[-0.05266883, -1.1479722 , -0.91910654, ..., -1.0627192 ,\n",
       "           1.8917779 , -3.2021184 ],\n",
       "         [-0.99960005, -1.814049  , -1.2755827 , ...,  0.17820004,\n",
       "           0.6803221 , -1.9258567 ],\n",
       "         [-0.5199583 , -0.96748495, -0.24519245, ...,  0.6883751 ,\n",
       "           1.5405521 , -0.56629604],\n",
       "         ...,\n",
       "         [-0.5716394 , -0.42393336, -0.50313747, ...,  1.702154  ,\n",
       "           0.2721098 , -1.1604007 ],\n",
       "         [-0.3273868 , -1.6059887 , -0.62930775, ...,  1.0949458 ,\n",
       "          -0.17370504, -0.71242684],\n",
       "         [-0.8578172 , -0.7463353 , -1.7864907 , ...,  0.81528056,\n",
       "           1.0453105 , -3.1349397 ]],\n",
       " \n",
       "        [[-0.6335479 , -1.5360096 , -1.2875961 , ..., -0.10443475,\n",
       "           2.3642352 , -1.9363225 ],\n",
       "         [-0.78554636, -1.0080341 , -0.94877857, ...,  0.31200022,\n",
       "           1.4780283 , -2.2557662 ],\n",
       "         [-0.9115924 , -1.8770783 , -0.56730795, ...,  0.7376658 ,\n",
       "           1.2151465 , -1.1365992 ],\n",
       "         ...,\n",
       "         [-1.4387406 , -1.4178362 , -0.66068965, ...,  1.7539084 ,\n",
       "          -0.01984975, -0.00411441],\n",
       "         [-2.246531  , -0.08452306, -1.2006652 , ...,  1.4465747 ,\n",
       "          -0.9637256 , -1.0263711 ],\n",
       "         [-1.8705947 , -0.8710522 ,  0.5137144 , ...,  1.0624017 ,\n",
       "          -0.27076238, -1.508417  ]],\n",
       " \n",
       "        [[-1.2745415 , -1.669393  , -1.8401873 , ...,  0.07825155,\n",
       "           0.48257262, -1.9440078 ],\n",
       "         [-1.1210157 , -1.7248046 , -1.6136177 , ...,  1.5010978 ,\n",
       "           0.255966  , -1.06436   ],\n",
       "         [-1.6086394 , -2.1262317 , -1.499492  , ...,  0.32461524,\n",
       "           0.4687978 , -1.578005  ],\n",
       "         ...,\n",
       "         [-1.2415868 , -1.4395006 , -1.0234599 , ...,  0.7297272 ,\n",
       "           1.0073227 , -1.3066903 ],\n",
       "         [-1.9026977 ,  0.12369786, -1.4928554 , ...,  0.37565655,\n",
       "          -0.33656234, -0.4545503 ],\n",
       "         [-1.9257642 , -0.93466604, -1.8551061 , ...,  0.53553843,\n",
       "          -0.01569953, -2.0327234 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.8994943 , -0.27319616, -0.9599721 , ...,  0.38641864,\n",
       "           0.8069746 , -1.632511  ],\n",
       "         [ 0.08660585, -0.25634974, -1.7741737 , ...,  1.1342121 ,\n",
       "           0.2301318 , -0.49294522],\n",
       "         [-0.5893241 , -1.0725731 , -0.6565964 , ...,  1.3974855 ,\n",
       "           0.78597724, -0.0251711 ],\n",
       "         ...,\n",
       "         [-1.1865754 , -0.36965266, -1.0329753 , ...,  0.98999447,\n",
       "          -0.09206203,  0.2372732 ],\n",
       "         [-1.751335  , -0.32731348, -1.1390814 , ...,  1.7086222 ,\n",
       "          -0.27255267,  0.8536342 ],\n",
       "         [-1.1671709 , -0.28584892, -0.34289542, ...,  1.1626185 ,\n",
       "          -0.87076837, -0.50674176]],\n",
       " \n",
       "        [[-1.5023483 , -0.68576705, -1.3155304 , ...,  0.2551629 ,\n",
       "          -0.12920368, -1.3617303 ],\n",
       "         [-1.402919  , -0.64698094, -0.4935171 , ..., -0.26048943,\n",
       "           0.53906417, -1.509345  ],\n",
       "         [-1.0948284 , -1.6794138 , -0.36381108, ...,  0.631464  ,\n",
       "           0.11678893, -1.7419428 ],\n",
       "         ...,\n",
       "         [-1.8943148 , -0.7043455 , -0.6834206 , ...,  1.9194807 ,\n",
       "          -0.5178222 , -1.0650227 ],\n",
       "         [-0.80844367, -1.1556025 , -0.5919905 , ...,  1.6843724 ,\n",
       "          -0.18083927, -0.45782286],\n",
       "         [-2.503589  , -0.30595973, -1.6583662 , ...,  1.2967135 ,\n",
       "           0.12535296, -0.7138746 ]],\n",
       " \n",
       "        [[-1.0534649 , -1.2609193 , -1.076131  , ...,  0.21532024,\n",
       "           1.4371812 , -1.6492026 ],\n",
       "         [-1.366441  , -1.893069  , -1.9875441 , ...,  1.0634972 ,\n",
       "           1.2623818 , -0.30727226],\n",
       "         [-0.6203355 , -1.5373261 , -1.1423365 , ...,  0.50171435,\n",
       "           0.9097145 , -1.7595559 ],\n",
       "         ...,\n",
       "         [-1.1758077 , -0.4475494 , -0.38236642, ...,  1.5317575 ,\n",
       "           0.2698564 , -0.7164807 ],\n",
       "         [-0.02857781, -0.7144181 , -1.0964473 , ...,  2.2967145 ,\n",
       "           1.6584055 , -0.79867244],\n",
       "         [-0.5677657 , -1.4931965 , -0.65402657, ...,  1.8337897 ,\n",
       "          -0.2565434 , -1.7025089 ]]], dtype=float32)>,\n",
       " 'dtme': <tf.Tensor: shape=(64, 80, 31), dtype=float32, numpy=\n",
       " array([[[-0.52644783, -0.76575404, -1.1036657 , ...,  0.13104077,\n",
       "           0.04669644,  1.6262323 ],\n",
       "         [-0.95915675,  0.00321724, -1.2665076 , ..., -0.14989488,\n",
       "           0.25914   ,  1.2640444 ],\n",
       "         [ 0.79774547, -1.1504642 , -1.5903883 , ..., -0.56555843,\n",
       "          -1.1155409 ,  1.3552372 ],\n",
       "         ...,\n",
       "         [ 0.7458043 ,  0.5541464 , -0.29890996, ..., -0.24798517,\n",
       "           0.28614232,  0.49340406],\n",
       "         [-0.394667  ,  0.6214641 , -0.3106572 , ...,  0.22430661,\n",
       "           0.99651796,  0.23346059],\n",
       "         [-1.3193852 ,  1.243921  , -1.2384381 , ..., -0.35718673,\n",
       "          -0.2922675 , -0.89454484]],\n",
       " \n",
       "        [[-0.553141  , -0.6768925 , -0.4666143 , ...,  1.5535454 ,\n",
       "           0.11717544,  1.1736109 ],\n",
       "         [-0.8694494 , -1.068928  , -0.9676058 , ...,  0.5763541 ,\n",
       "           0.36087722,  0.79733914],\n",
       "         [-1.4844272 , -1.1335069 , -0.9807048 , ...,  0.2716545 ,\n",
       "           0.17435734,  1.2710083 ],\n",
       "         ...,\n",
       "         [-1.0242308 ,  0.0671633 , -1.4774191 , ...,  0.51881194,\n",
       "           0.82474685,  0.14548741],\n",
       "         [-1.712399  ,  1.0468719 , -0.8589957 , ...,  0.39262888,\n",
       "           1.5495784 ,  0.43022352],\n",
       "         [-1.7813605 ,  0.43207553, -1.8828988 , ...,  0.5334811 ,\n",
       "           0.9575272 ,  0.05537418]],\n",
       " \n",
       "        [[-0.70017076,  0.5907606 , -0.8885461 , ...,  0.72628164,\n",
       "           0.04165838,  0.89096314],\n",
       "         [-0.86072594, -0.4793645 , -1.3467582 , ...,  0.22810529,\n",
       "          -0.8799441 ,  1.0776821 ],\n",
       "         [-0.86626667,  0.24313962, -1.2961429 , ...,  0.5905096 ,\n",
       "          -0.5568555 ,  1.3060143 ],\n",
       "         ...,\n",
       "         [-0.10931916, -0.6677675 , -1.320301  , ...,  0.91406   ,\n",
       "           0.23333173,  0.30605406],\n",
       "         [-0.19781712,  0.73350346, -1.4012015 , ..., -0.29668966,\n",
       "          -0.05643666, -0.16015524],\n",
       "         [-2.3492885 , -0.07985048, -0.7083142 , ...,  0.84882337,\n",
       "           1.2105838 ,  0.6614466 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.43073097, -1.0839742 ,  0.28198197, ...,  0.86071783,\n",
       "           0.654806  ,  2.0445092 ],\n",
       "         [-0.7996334 , -0.18491307, -0.07672513, ...,  0.23217428,\n",
       "          -0.64408344,  1.4817368 ],\n",
       "         [-1.1046364 ,  0.07875293, -0.81582344, ...,  0.5812455 ,\n",
       "          -0.5254246 ,  2.040238  ],\n",
       "         ...,\n",
       "         [ 0.45158443,  1.3526666 , -1.646483  , ..., -0.34605798,\n",
       "           0.27364722,  0.65551364],\n",
       "         [ 0.29126728,  0.8892197 , -1.7444587 , ...,  0.2730513 ,\n",
       "           0.62581134,  0.3946206 ],\n",
       "         [ 0.44106597,  0.2686166 , -1.9146377 , ...,  0.7132299 ,\n",
       "          -0.18211333,  1.1236378 ]],\n",
       " \n",
       "        [[-0.8751007 ,  0.332462  , -1.2727629 , ..., -0.60217935,\n",
       "           1.4982512 ,  0.5832976 ],\n",
       "         [-0.41418847, -0.4252003 , -0.9572764 , ..., -0.02836173,\n",
       "           0.18791386,  0.74209017],\n",
       "         [-1.1349438 ,  0.02423999, -0.7467095 , ...,  0.06323777,\n",
       "           0.94157505,  1.6342667 ],\n",
       "         ...,\n",
       "         [-0.42573294,  0.16899706, -0.618986  , ...,  1.2944862 ,\n",
       "           1.012352  ,  0.9038943 ],\n",
       "         [-0.0350901 ,  0.12238271, -0.6555017 , ...,  0.8271019 ,\n",
       "           0.33259216,  0.42949325],\n",
       "         [-0.9481946 ,  0.48995608, -0.7521665 , ...,  0.1932694 ,\n",
       "           1.0580113 ,  1.1018324 ]],\n",
       " \n",
       "        [[-0.5792992 ,  0.21771121, -0.571664  , ..., -0.17271309,\n",
       "          -0.51026434,  2.4082634 ],\n",
       "         [-0.50691974,  0.51613206, -0.6116561 , ..., -0.38360512,\n",
       "          -1.2575877 ,  1.3196187 ],\n",
       "         [-0.45398775,  1.2268184 , -0.7371555 , ..., -1.1173803 ,\n",
       "          -0.43899453,  1.8841695 ],\n",
       "         ...,\n",
       "         [-0.7490086 ,  0.7494419 ,  0.06258174, ..., -0.07571019,\n",
       "          -0.13930231,  2.3579667 ],\n",
       "         [-0.6052752 , -0.5739699 ,  0.82094765, ...,  0.20352224,\n",
       "          -0.7508031 ,  1.751961  ],\n",
       "         [-1.7241795 ,  0.9563386 ,  0.611964  , ...,  0.6381538 ,\n",
       "          -0.51246905,  2.2381694 ]]], dtype=float32)>,\n",
       " 'td_sc': <tf.Tensor: shape=(64, 80, 2), dtype=float32, numpy=\n",
       " array([[[0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.01298337],\n",
       "         [0.69317514, 0.        ],\n",
       "         [0.        , 0.        ]],\n",
       " \n",
       "        [[0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ]],\n",
       " \n",
       "        [[0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.10778056],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ]],\n",
       " \n",
       "        [[0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ]],\n",
       " \n",
       "        [[0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.03746053],\n",
       "         [0.        , 0.        ],\n",
       "         [0.        , 0.        ]]], dtype=float32)>,\n",
       " 'log_amount_sc': <tf.Tensor: shape=(64, 80, 2), dtype=float32, numpy=\n",
       " array([[[1.7522107 , 1.3557695 ],\n",
       "         [0.60265887, 0.49250573],\n",
       "         [1.3325211 , 0.64946866],\n",
       "         ...,\n",
       "         [1.0138783 , 0.10927854],\n",
       "         [0.34278545, 0.        ],\n",
       "         [1.1428564 , 0.02685582]],\n",
       " \n",
       "        [[0.9028988 , 0.        ],\n",
       "         [0.99355394, 0.        ],\n",
       "         [0.06502914, 0.01064545],\n",
       "         ...,\n",
       "         [0.6271261 , 0.45853126],\n",
       "         [0.38964772, 0.        ],\n",
       "         [0.91112685, 0.2910519 ]],\n",
       " \n",
       "        [[1.278181  , 0.        ],\n",
       "         [0.5912692 , 0.        ],\n",
       "         [1.2370443 , 0.        ],\n",
       "         ...,\n",
       "         [1.4329348 , 0.01085325],\n",
       "         [1.7089231 , 0.        ],\n",
       "         [1.4992017 , 0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.6019613 , 0.4826052 ],\n",
       "         [0.62701285, 0.69076025],\n",
       "         [0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.3633096 , 0.46063983],\n",
       "         [0.1667538 , 0.13517942],\n",
       "         [0.6613232 , 0.        ]],\n",
       " \n",
       "        [[0.6445938 , 0.        ],\n",
       "         [0.41146985, 0.        ],\n",
       "         [0.8246148 , 0.        ],\n",
       "         ...,\n",
       "         [0.73810667, 0.        ],\n",
       "         [0.99277353, 0.        ],\n",
       "         [0.25193515, 0.        ]],\n",
       " \n",
       "        [[0.5442436 , 0.09789474],\n",
       "         [0.20291503, 0.49618742],\n",
       "         [0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.33919036, 0.        ],\n",
       "         [0.7744505 , 0.        ],\n",
       "         [1.166779  , 0.        ]]], dtype=float32)>}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tcode_num': 0,\n",
       "  'dow': 1,\n",
       "  'month': 2,\n",
       "  'day': 3,\n",
       "  'dtme': 4,\n",
       "  'td_sc': 5,\n",
       "  'log_amount_sc': 6},\n",
       " {'tcode_num': 1,\n",
       "  'dow': 1,\n",
       "  'month': 1,\n",
       "  'day': 1,\n",
       "  'dtme': 1,\n",
       "  'td_sc': 1,\n",
       "  'log_amount_sc': 1})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_STARTS_TAR, FIELD_DIMS_TAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': 0,\n",
       " 'dow': 16,\n",
       " 'month': 18,\n",
       " 'day': 20,\n",
       " 'dtme': 22,\n",
       " 'td_sc': 24,\n",
       " 'log_amount_sc': 25}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_STARTS_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcode_num': 16,\n",
       " 'dow': 2,\n",
       " 'month': 2,\n",
       " 'day': 2,\n",
       " 'dtme': 2,\n",
       " 'td_sc': 1,\n",
       " 'log_amount_sc': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_DIMS_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 'scce',\n",
       " 'dtme': 'scce',\n",
       " 'dow': 'scce',\n",
       " 'month': 'scce',\n",
       " 'tcode_num': 'scce',\n",
       " 'td_sc': 'pdf',\n",
       " 'log_amount_sc': 'pdf'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOSS_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n",
    "real = tar\n",
    "loss_parts = []\n",
    "loss_parts_weighted = []\n",
    "LOSS_WEIGHTS = {\n",
    " 'td_sc':1.,\n",
    " 'month': 0.015,\n",
    " 'day': 0.025,\n",
    " 'dtme': 0.025,\n",
    " 'dow': 0.01,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "for k, k_pred in predictions.items():\n",
    "\n",
    "    st = FIELD_STARTS_TAR[k]\n",
    "    end = st + FIELD_DIMS_TAR[k]\n",
    "    loss_type = LOSS_TYPES[k]\n",
    "    \n",
    "\n",
    "    if loss_type == \"scce\":\n",
    "        loss_ = SparseCategoricalCrossentropy(from_logits=True, reduction='none')(real[:, :, st:end], k_pred)   #(batch_size, seq_len)\n",
    "        \n",
    "    elif loss_type == \"pdf\":\n",
    "        loss_ = -log_normal_pdf(real[:, :, st:end], k_pred[:,:,0:1], k_pred[:,:,1:2])[:,:,0]                     #(batch_size, seq_len)\n",
    "    mask = tf.math.logical_not(tf.math.equal(tf.reduce_sum(real, axis=2), 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "    loss_parts.append(loss_)\n",
    "    loss_parts_weighted.append(loss_ * LOSS_WEIGHTS[k])\n",
    "loss = tf.reduce_sum(loss_parts_weighted)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=13.687483>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "n_seqs_to_generate= 50\n",
    "start_date_opts = data.groupby(\"account_id\")[\"datetime\"].min().dt.date.to_list()\n",
    "start_dates = np.random.choice(start_date_opts, size=n_seqs_to_generate) # sample start dates from real data\n",
    "seq_ages = np.random.choice(attributes, size=n_seqs_to_generate) # sample ages from real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df, seqs, raw = generate_seqs(length= seq_len, ages=seq_ages, start_dates= start_dates, return_single_df=True, greedy_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "preds = {}\n",
    "\n",
    "for name, dim in FIELD_DIMS_NET.items():\n",
    "    acti = ACTIVATIONS.get(name, None)\n",
    "    layers_dict[name] = tf.keras.layers.Dense(dim, activation=acti)\n",
    "final_output2 = final_output\n",
    "for net_name in DATA_KEY_ORDER:\n",
    "    #print(\"Running net\", net_name)\n",
    "    pred = layers_dict[net_name](final_output2)\n",
    "    #print(\"pred shape\", pred.shape)\n",
    "    preds[net_name] = pred\n",
    "    st = FIELD_STARTS_IN[net_name]\n",
    "    end = st + FIELD_DIMS_IN[net_name]\n",
    "    to_add = tar_out[:, :, st: end]\n",
    "    #print(\"Start and end\", st, end)\n",
    "            \n",
    "    final_output2 = tf.concat([final_output2, to_add], axis=-1)\n",
    "    #print(\"Final output shape after\",net_name, \"is\", final_output.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqs_to_generate= 50\n",
    "start_date_opts = data.groupby(\"account_id\")[\"datetime\"].min().dt.date.to_list()\n",
    "start_dates = np.random.choice(start_date_opts, size=n_seqs_to_generate) # sample start dates from real data\n",
    "seq_ages = np.random.choice(attributes, size=n_seqs_to_generate) # sample ages from real data\n",
    "#full_df, seqs, raw = generate_seqs(length= seq_len, ages=seq_ages, start_dates= start_dates, return_single_df=True, greedy_dates=True)\n",
    "# the body of function generate_seqs()\n",
    "length = max_seq_len\n",
    "ages = seq_ages\n",
    "start_dates = start_dates\n",
    "greedy_dates = True\n",
    "return_single_df = True\n",
    "return_df_list=False\n",
    "if return_single_df and return_df_list:\n",
    "    raise Exception(\"At most one of: 'return_single_df' and 'return_df_list' can be true\")\n",
    "START_DATE = df[\"datetime\"].min()\n",
    "START_DATE = START_DATE.date()\n",
    "date_inds = np.array([(d - START_DATE).days for d in start_dates])\n",
    "max_length = length\n",
    "outp = np.repeat(np.array(ages)[:, None, None], repeats=n_feat_inp, axis=2) / ATTR_SCALE    #(n_seqs_to_generate, 1, n_feat_inp)\n",
    "raw_preds = []\n",
    "raw_preds.append(outp)\n",
    "\n",
    "date_info = None\n",
    "\n",
    "i = 0 #(for i in range(max_length))\n",
    "combined_mask, dec_padding_mask = create_masks(outp) \n",
    "#call_to_generate(transformer, outp, True, combined_mask, dec_padding_mask, date_inds, date_info, greedy_dates =greedy_dates)\n",
    "\n",
    "transformer, attention_weights = createmodel(inp_dim, dff, d_model, rate, num_heads, combined_mask)\n",
    "final_output = transformer(outp)\n",
    "\n",
    "### Predict each field  ###\n",
    "preds_dict = {}\n",
    "raw_preds_dict = {}\n",
    "encoded_preds_d = {}\n",
    "encoded_preds = []\n",
    "\n",
    "layers_dict = {}\n",
    "for name, dim in FIELD_DIMS_NET.items():\n",
    "    acti = ACTIVATIONS.get(name, None)\n",
    "    layers_dict[name] = tf.keras.layers.Dense(dim, activation=acti) \n",
    "for net_name in DATA_KEY_ORDER:\n",
    "    #print(\"Running net\", net_name)\n",
    "    pred = layers_dict[net_name](final_output)\n",
    "    #print(\"pred shape\", pred.shape)\n",
    "    raw_preds_dict[net_name] = pred\n",
    "    pred = reencode_net_prediction(net_name, pred) # keeps time step\n",
    "    preds_dict[net_name] = pred\n",
    "    encoded_preds_d[net_name] = pred[:,-1,:] \n",
    "    encoded_preds.append(pred[:,-1,:])  \n",
    "    final_output = tf.concat([final_output, pred], axis=2)\n",
    "\n",
    "\n",
    "encoded_preds_d\n",
    "      \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 3, 26), dtype=float32, numpy=\n",
       "array([[[ 4.273525  ,  4.273525  ,  4.273525  , ...,  4.273525  ,\n",
       "          4.273525  ,  4.273525  ],\n",
       "        [ 0.        ,  1.        ,  0.        , ...,  0.9795299 ,\n",
       "          2.6385605 ,  0.5842118 ],\n",
       "        [ 0.        ,  1.        ,  0.        , ...,  0.9795299 ,\n",
       "          2.6385605 ,  0.5842118 ]],\n",
       "\n",
       "       [[ 2.6483817 ,  2.6483817 ,  2.6483817 , ...,  2.6483817 ,\n",
       "          2.6483817 ,  2.6483817 ],\n",
       "        [ 0.        ,  1.        ,  0.        , ...,  0.34730524,\n",
       "          1.6491003 ,  0.08843018],\n",
       "        [ 0.        ,  1.        ,  0.        , ...,  0.34730524,\n",
       "          1.6491003 ,  0.08843018]],\n",
       "\n",
       "       [[ 2.4076197 ,  2.4076197 ,  2.4076197 , ...,  2.4076197 ,\n",
       "          2.4076197 ,  2.4076197 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.15142778,\n",
       "          1.8140104 ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.15142778,\n",
       "          1.8140104 ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.2038099 ,  1.2038099 ,  1.2038099 , ...,  1.2038099 ,\n",
       "          1.2038099 ,  1.2038099 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.68896693,\n",
       "          2.3087404 ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.68896693,\n",
       "          2.3087404 ,  0.        ]],\n",
       "\n",
       "       [[ 3.4910486 ,  3.4910486 ,  3.4910486 , ...,  3.4910486 ,\n",
       "          3.4910486 ,  3.4910486 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.44039416,\n",
       "          6.7613115 ,  0.28275937],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.44039416,\n",
       "          6.7613115 ,  0.28275937]],\n",
       "\n",
       "       [[ 1.6251434 ,  1.6251434 ,  1.6251434 , ...,  1.6251434 ,\n",
       "          1.6251434 ,  1.6251434 ],\n",
       "        [ 1.        ,  0.        ,  0.        , ..., -0.61210597,\n",
       "          3.6280208 ,  0.        ],\n",
       "        [ 1.        ,  0.        ,  0.        , ..., -0.61210597,\n",
       "          3.6280208 ,  0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combined_date_info, inds = raw_dates_to_reencoded(raw_preds_dict, date_inds)\n",
    "import datetime\n",
    "import calendar\n",
    "get_dtme = lambda d: calendar.monthrange(d.year, d.month)[1] - d.day\n",
    "MAX_YEARS_SPAN = 15\n",
    "END_DATE = START_DATE.replace(year = START_DATE.year+ MAX_YEARS_SPAN)\n",
    "ALL_DATES = [START_DATE + datetime.timedelta(i) for i in range((END_DATE - START_DATE).days)]\n",
    "AD = np.array([(d.month % 12, d.day % 31, d.weekday() % 7, i, d.year, get_dtme(d)) for i, d in enumerate(ALL_DATES)])\n",
    "max_days = 100\n",
    "all_ps = [tf.nn.softmax(raw_preds_dict[k][:,-1]).numpy() for k in [\"month\", \"day\", \"dow\", \"dtme\"]]   #(n_seqs_to_generate, datedim)\n",
    "timesteps = np.zeros(len(date_inds)).astype(int)\n",
    "sc = TD_SCALE\n",
    "for i, (month_ps, day_ps, dow_ps, dtme_ps, td_pred, si) in enumerate(zip(*all_ps, raw_preds_dict[\"td_sc\"][:,-1].numpy(), date_inds)):\n",
    "        ps = month_ps[AD[si:si+max_days,0]]*day_ps[AD[si:si+max_days,1]]*dow_ps[AD[si:si+max_days,2]] *dtme_ps[AD[si:si+max_days,-1]] * \\\n",
    "                np.exp(log_normal_pdf_gen(AD[si:si+max_days,3]-si, mean = td_pred[0]*sc, logvar=td_pred[1]*sc))\n",
    "        if greedy_dates:\n",
    "            timesteps[i] = np.argmax(ps)\n",
    "        else:\n",
    "            timesteps[i] = np.random.choice(max_days, p=ps/sum(ps))\n",
    "inds = date_inds + timesteps\n",
    "\n",
    "combined_date_info = {}\n",
    "combined_date_info[\"td_sc\"] = tf.expand_dims(timesteps.astype(np.float32)/ TD_SCALE, axis=1)\n",
    "combined_date_info[\"month\"] = bulk_encode_time_value(AD[inds, 0], 12)\n",
    "combined_date_info[\"day\"] = bulk_encode_time_value(AD[inds, 1], 31)\n",
    "combined_date_info[\"dow\"] = bulk_encode_time_value(AD[inds, 2], 7)\n",
    "combined_date_info[\"dtme\"] = bulk_encode_time_value(AD[inds, -1], 31)\n",
    "\n",
    "encoded_preds_d.update(combined_date_info)\n",
    "l = [encoded_preds_d[k] for k in DATA_KEY_ORDER]\n",
    "encoded_preds =  tf.expand_dims(tf.concat(l, axis=1), axis=1)    #(n_seqs_to_generate, 1, 26)\n",
    "enc_preds = tf.reshape(tf.constant(encoded_preds), shape=(-1,1, n_feat_inp))\n",
    "outp =  tf.concat([outp, enc_preds], axis=1)\n",
    "outp\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 1, 26), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  1.        ,  0.        , ...,  0.9795299 ,\n",
       "          2.6385605 ,  0.5842118 ]],\n",
       "\n",
       "       [[ 0.        ,  1.        ,  0.        , ...,  0.34730524,\n",
       "          1.6491003 ,  0.08843018]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.15142778,\n",
       "          1.8140104 ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.68896693,\n",
       "          2.3087404 ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ..., -0.44039416,\n",
       "          6.7613115 ,  0.28275937]],\n",
       "\n",
       "       [[ 1.        ,  0.        ,  0.        , ..., -0.61210597,\n",
       "          3.6280208 ,  0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 1, 26), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  1.        ,  0.        , ...,  0.9795299 ,\n",
       "          2.6385605 ,  0.5842118 ]],\n",
       "\n",
       "       [[ 0.        ,  1.        ,  0.        , ...,  0.34730524,\n",
       "          1.6491003 ,  0.08843018]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.15142778,\n",
       "          1.8140104 ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.68896693,\n",
       "          2.3087404 ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ..., -0.44039416,\n",
       "          6.7613115 ,  0.28275937]],\n",
       "\n",
       "       [[ 1.        ,  0.        ,  0.        , ..., -0.61210597,\n",
       "          3.6280208 ,  0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_preds_d.update(combined_date_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf_gen(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.cast(tf.math.log(2. * np.pi), tf.float64)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used when generating data\n",
    "# Takes the output from BF and transforms it into the \n",
    "# correct format to be used as input to BF\n",
    "def reencode_net_prediction(net_name, predictions):\n",
    "    \n",
    "    date_info = {'month':12, 'day':31, 'dtme':31, 'dow':7}\n",
    "    batch_size = predictions.shape[0]                      ##(n_seqs_to_generate, 1, 1)\n",
    "    \n",
    "    \n",
    "    if net_name in ['td_sc', \"log_amount_sc\"]:\n",
    "        return predictions[:, :, 0:1]\n",
    "    \n",
    "\n",
    "\n",
    "    elif net_name in date_info.keys():\n",
    "        dim = FIELD_DIMS_NET[net_name]\n",
    "        choices = np.arange(dim)\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, dim)    #(n_seqs_to_generate, dim of date info)\n",
    "        \n",
    "        \n",
    "        choosen =  np.array([np.random.choice(choices, p=p) for p in ps])   #(n_seqs_to_generate,)\n",
    "        \n",
    "        x = bulk_encode_time_value(choosen, max_val=dim)            #(n_seqs_to_generate, 2)\n",
    "        \n",
    "        return np.reshape(x, newshape=(batch_size, -1, 2))          #(n_seqs_to_generate, 1, 2)\n",
    "\n",
    "    \n",
    "    elif \"_num\" in net_name:\n",
    "        dim = FIELD_DIMS_NET[net_name]\n",
    "        choices = np.arange(dim)\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, dim)  #(n_seqs_to_generate, 16)\n",
    "\n",
    "        \n",
    "        choosen =  np.reshape([np.random.choice(choices, p=p) for p in ps], newshape=(batch_size, -1))   #(n_seqs_to_generate, 1)\n",
    "        \n",
    "        return tf.one_hot(choosen, depth=dim)   #(n_seqs_to_generate, 1, 16)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f\"Got invalid net_name: {net_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel(inp_dim, dff, d_model, rate, num_heads, combined_mask):\n",
    "    #input layer\n",
    "    inp = tf.keras.layers.Input((None, inp_dim))           #(batch_size, seq_len, inp_feature)\n",
    "    x = tf.keras.layers.Dense(dff, activation='relu')(inp)       #(batch_size, seq_len, dff=128)\n",
    "    x = tf.keras.layers.Dense(d_model)(x)                        # (batch_size, seq_len, d_model=128)\n",
    "    #seq_len = tf.shape(x)[1]\n",
    "    #seq_len = 80\n",
    "    attention_weights_dic = {}\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    x += pos_encoding[:, :seq_len, :]     #x is the output of Input layer\n",
    "    x = tf.keras.layers.Dropout(rate)(x, training=True)\n",
    "    #Decoder Layer\n",
    "    #Decoder Block1\n",
    "    ## MultiHeadAttention\n",
    "    assert d_model % num_heads == 0\n",
    "    depth = d_model // num_heads\n",
    "    q = tf.keras.layers.Dense(d_model)(x)    #Wq\n",
    "    k = tf.keras.layers.Dense(d_model)(x)    #Wk\n",
    "    v = tf.keras.layers.Dense(d_model)(x)    #Wv\n",
    "\n",
    "    #batch_size = bs\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    q = tf.reshape(q, (batch_size, -1, num_heads, depth))\n",
    "    q = tf.transpose(q, perm=[0,2,1,3])                    #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    k = tf.reshape(k, (batch_size, -1, num_heads, depth))\n",
    "    k = tf.transpose(k, perm=[0,2,1,3])                     #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    v = tf.reshape(v, (batch_size, -1, num_heads, depth))\n",
    "    v = tf.transpose(v, perm=[0,2,1,3])                      #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)     #scores  (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    # scale scores(matmul_qk)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)     #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    # add the mask to the scaled tensor\n",
    "    scaled_attention_logits += (combined_mask * -1e9)           #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    scaled_attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, d_model))  # (batch_size, seq_len_q, d_model)\n",
    "    output = tf.keras.layers.Dense(d_model)(concat_attention)\n",
    "\n",
    "\n",
    "    attn_weights_block1 = attention_weights\n",
    "    attn1 = output\n",
    "\n",
    "    attn1 = tf.keras.layers.Dropout(rate)(attn1, training=True)\n",
    "    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn1 + x)    #Add & Norm\n",
    "\n",
    "    #point-wise feed forward network\n",
    "    ffn_output = tf.keras.layers.Dense(dff, activation='relu')(out1)  # (batch_size, seq_len, dff)\n",
    "    ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)  # (batch_size, seq_len, d_model)\n",
    "    ffn_output = tf.keras.layers.Dropout(rate)(ffn_output, training=True)\n",
    "\n",
    "    out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_output+out1)   # (batch_size, seq_len, d_model)\n",
    "\n",
    "    i = 0\n",
    "    attention_weights_dic['decoder_layer{}_block1'.format(i+1)] = attn_weights_block1\n",
    "\n",
    "\n",
    "    #Decoder Block2\n",
    "    x = out3\n",
    "    q = tf.keras.layers.Dense(d_model)(x)    #Wq\n",
    "    k = tf.keras.layers.Dense(d_model)(x)    #Wk\n",
    "    v = tf.keras.layers.Dense(d_model)(x)    #Wv\n",
    "\n",
    "    #batch_size = bs\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    q = tf.reshape(q, (batch_size, -1, num_heads, depth))\n",
    "    q = tf.transpose(q, perm=[0,2,1,3])                    #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    k = tf.reshape(k, (batch_size, -1, num_heads, depth))\n",
    "    k = tf.transpose(k, perm=[0,2,1,3])                     #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    v = tf.reshape(v, (batch_size, -1, num_heads, depth))\n",
    "    v = tf.transpose(v, perm=[0,2,1,3])                      #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)     #scores  (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    # scale scores(matmul_qk)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)     #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    # add the mask to the scaled tensor\n",
    "    scaled_attention_logits += (combined_mask * -1e9)           #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    scaled_attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, d_model))  # (batch_size, seq_len_q, d_model)\n",
    "    output = tf.keras.layers.Dense(d_model)(concat_attention)\n",
    "\n",
    "\n",
    "    attn_weights_block2 = attention_weights\n",
    "    attn1 = output\n",
    "\n",
    "    attn1 = tf.keras.layers.Dropout(rate)(attn1, training=True)\n",
    "    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn1 + x)    #Add & Norm\n",
    "\n",
    "    #point-wise feed forward network\n",
    "    ffn_output = tf.keras.layers.Dense(dff, activation='relu')(out1)  # (batch_size, seq_len, dff)\n",
    "    ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)  # (batch_size, seq_len, d_model)\n",
    "    ffn_output = tf.keras.layers.Dropout(rate)(ffn_output, training=True)\n",
    "\n",
    "    out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_output+out1)   # (batch_size, seq_len, d_model)\n",
    "\n",
    "    i = 1\n",
    "    attention_weights_dic['decoder_layer{}_block1'.format(i+1)] = attn_weights_block2\n",
    "\n",
    "    #Decoder Block3\n",
    "    x = out3\n",
    "    q = tf.keras.layers.Dense(d_model)(x)    #Wq\n",
    "    k = tf.keras.layers.Dense(d_model)(x)    #Wk\n",
    "    v = tf.keras.layers.Dense(d_model)(x)    #Wv\n",
    "\n",
    "    #batch_size = bs\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    q = tf.reshape(q, (batch_size, -1, num_heads, depth))\n",
    "    q = tf.transpose(q, perm=[0,2,1,3])                    #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    k = tf.reshape(k, (batch_size, -1, num_heads, depth))\n",
    "    k = tf.transpose(k, perm=[0,2,1,3])                     #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    v = tf.reshape(v, (batch_size, -1, num_heads, depth))\n",
    "    v = tf.transpose(v, perm=[0,2,1,3])                      #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)     #scores  (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    # scale scores(matmul_qk)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)     #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    # add the mask to the scaled tensor\n",
    "    scaled_attention_logits += (combined_mask * -1e9)           #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    scaled_attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, d_model))  # (batch_size, seq_len_q, d_model)\n",
    "    output = tf.keras.layers.Dense(d_model)(concat_attention)\n",
    "\n",
    "\n",
    "    attn_weights_block3 = attention_weights\n",
    "    attn1 = output\n",
    "\n",
    "    attn1 = tf.keras.layers.Dropout(rate)(attn1, training=True)\n",
    "    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn1 + x)    #Add & Norm\n",
    "\n",
    "    #point-wise feed forward network\n",
    "    ffn_output = tf.keras.layers.Dense(dff, activation='relu')(out1)  # (batch_size, seq_len, dff)\n",
    "    ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)  # (batch_size, seq_len, d_model)\n",
    "    ffn_output = tf.keras.layers.Dropout(rate)(ffn_output, training=True)\n",
    "\n",
    "    out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_output+out1)   # (batch_size, seq_len, d_model)\n",
    "\n",
    "    i = 2\n",
    "    attention_weights_dic['decoder_layer{}_block1'.format(i+1)] = attn_weights_block3\n",
    "\n",
    "    #Decoder Block4\n",
    "    x = out3\n",
    "    q = tf.keras.layers.Dense(d_model)(x)    #Wq\n",
    "    k = tf.keras.layers.Dense(d_model)(x)    #Wk\n",
    "    v = tf.keras.layers.Dense(d_model)(x)    #Wv\n",
    "\n",
    "    #batch_size = bs\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    q = tf.reshape(q, (batch_size, -1, num_heads, depth))\n",
    "    q = tf.transpose(q, perm=[0,2,1,3])                    #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    k = tf.reshape(k, (batch_size, -1, num_heads, depth))\n",
    "    k = tf.transpose(k, perm=[0,2,1,3])                     #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    v = tf.reshape(v, (batch_size, -1, num_heads, depth))\n",
    "    v = tf.transpose(v, perm=[0,2,1,3])                      #(batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)     #scores  (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    # scale scores(matmul_qk)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)     #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    # add the mask to the scaled tensor\n",
    "    scaled_attention_logits += (combined_mask * -1e9)           #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)    #(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    scaled_attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, d_model))  # (batch_size, seq_len_q, d_model)\n",
    "    output = tf.keras.layers.Dense(d_model)(concat_attention)\n",
    "\n",
    "\n",
    "    attn_weights_block4 = attention_weights\n",
    "    attn1 = output\n",
    "\n",
    "    attn1 = tf.keras.layers.Dropout(rate)(attn1, training=True)\n",
    "    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn1 + x)    #Add & Norm\n",
    "\n",
    "    #point-wise feed forward network\n",
    "    ffn_output = tf.keras.layers.Dense(dff, activation='relu')(out1)  # (batch_size, seq_len, dff)\n",
    "    ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)  # (batch_size, seq_len, d_model)\n",
    "    ffn_output = tf.keras.layers.Dropout(rate)(ffn_output, training=True)\n",
    "\n",
    "    out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ffn_output+out1)   # (batch_size, seq_len, d_model)\n",
    "\n",
    "    i = 3\n",
    "    attention_weights_dic['decoder_layer{}_block1'.format(i+1)] = attn_weights_block4\n",
    "\n",
    "    dec_output = out3\n",
    "\n",
    "    final_output = tf.keras.layers.Dense(d_model, activation=None)(dec_output)\n",
    "    model_pred = tf.keras.models.Model(inp , final_output)\n",
    "    return model_pred, attention_weights_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "mainenv"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
